// <auto-generated>
// This file was generated by ModularPipelines.OptionsGenerator on 2025-12-23.
// Source: https://cloud.google.com/sdk/gcloud/reference/dataplex/datascans/update/data-discovery
// Do not edit this file manually.
// </auto-generated>

#nullable enable

using System.Diagnostics.CodeAnalysis;
using ModularPipelines.Attributes;
using ModularPipelines.Google.Options;
using ModularPipelines.Models;

namespace ModularPipelines.Google.Options;

/// <summary>
/// update a Dataplex data      discovery scan job
/// </summary>
[ExcludeFromCodeCoverage]
[CliSubCommand("dataplex", "datascans", "update", "data-discovery")]
public record GcloudDataplexDatascansUpdateDataDiscoveryOptions : GcloudOptions
{
    /// <summary>
    /// Description of the data discovery scan
    /// </summary>
    [CliOption("--description", Format = OptionFormat.EqualsSeparated)]
    public string? Description { get; set; }

    /// <summary>
    /// Display name of the data discovery scan
    /// </summary>
    [CliOption("--display-name", Format = OptionFormat.EqualsSeparated)]
    public string? DisplayName { get; set; }

    /// <summary>
    /// List of label KEY=VALUE pairs to add.        Keys must start with a lowercase character and contain only hyphens      (-), underscores (_), lowercase characters, and numbers. Values must      contain only hyphens (-), underscores (_), lowercase characters, and      numbers.      At most one of --async | --validate-only can be specified.      At most one of these can be specified:       --async       Return immediately, without waiting for the operation in progress to       complete.       --validate-only       Validate the update action, but don't actually perform it.      Data spec for the data discovery scan.       BigQuery publishing config arguments for the data discovery scan.        --bigquery-publishing-connection=BIGQUERY_PUBLISHING_CONNECTION        BigQuery connection to use for auto discovering cloud resource        bucket to BigLake tables. Connection is required for        BIGLAKE`BigQuery publishing table type.        --bigquery-publishing-dataset-location=BIGQUERY_PUBLISHING_DATASET_LOCATION        The location of the BigQuery dataset to publish BigLake external or        non-BigLake external tables to. If not specified, the dataset        location will be set to the location of the data source resource.        Refer to        https://cloud.google.com/bigquery/docs/locations#supportedLocations        for supported locations.        --bigquery-publishing-dataset-project=BIGQUERY_PUBLISHING_DATASET_PROJECT        The project of the BigQuery dataset to publish BigLake external or        non-BigLake external tables to. If not specified, the cloud        resource bucket project will be used to create the dataset. The        format is "projects/{project_id_or_number}.        --bigquery-publishing-table-type=BIGQUERY_PUBLISHING_TABLE_TYPE        BigQuery table type to discover the cloud resource bucket. Can be        either EXTERNAL or BIGLAKE. If not specified, the table type will        be set to EXTERNAL.       Storage config arguments for the data discovery scan.        --storage-exclude-patterns=[PATTERN,...]        List of patterns that identify the data to exclude during        discovery. These patterns are interpreted as glob patterns used to        match object names in the Cloud Storage bucket. Exclude patterns        will be applied before include patterns.        --storage-include-patterns=[PATTERN,...]        List of patterns that identify the data to include during discovery        when only a subset of the data should be considered. These patterns        are interpreted as glob patterns used to match object names in the        Cloud Storage bucket.        CSV options arguments for the data discovery scan.         --csv-delimiter=CSV_DELIMITER         Delimiter used to separate values in the CSV file. If not         specified, the delimiter will be set to comma (",").         --csv-disable-type-inference=CSV_DISABLE_TYPE_INFERENCE         Whether to disable the inference of data types for CSV data. If         true, all columns are registered as strings.         --csv-encoding=CSV_ENCODING         Character encoding of the CSV file. If not specified, the         encoding will be set to UTF-8.         --csv-header-row-count=CSV_HEADER_ROW_COUNT         The number of rows to interpret as header rows that should be         skipped when reading data rows. The default value is 1.         --csv-quote-character=CSV_QUOTE_CHARACTER         The character used to quote column values. Accepts " (double         quotation mark) or ' (single quotation mark). If unspecified,         defaults to " (double quotation mark).        JSON options arguments for the data discovery scan.         --json-disable-type-inference=JSON_DISABLE_TYPE_INFERENCE         Whether to disable the inference of data types for JSON data. If         true, all columns are registered as strings.         --json-encoding=JSON_ENCODING         Character encoding of the JSON file. If not specified, the         encoding will be set to UTF-8.      Data discovery scan execution settings.       Data discovery scan scheduling and trigger settings       At most one of these can be specified:        --on-demand=ON_DEMAND        If set, the scan runs one-time shortly after data discovery scan        updation.        --schedule=SCHEDULE        Cron schedule (https://en.wikipedia.org/wiki/Cron) for running        scans periodically. To explicitly set a timezone to the cron tab,        apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or        "TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid        string from IANA time zone database. For example,        CRON_TZ=America/New_York 1 * * * * or TZ=America/New_York 1 * * *        *. This field is required for RECURRING scans.
    /// </summary>
    [CliOption("--labels", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Labels { get; set; }

}
