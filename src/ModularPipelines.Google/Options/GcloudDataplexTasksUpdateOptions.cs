// <auto-generated>
// This file was generated by ModularPipelines.OptionsGenerator on 2025-12-28.
// Source: https://cloud.google.com/sdk/gcloud/reference/dataplex/tasks/update
// Do not edit this file manually.
// </auto-generated>

#nullable enable

using System.Diagnostics.CodeAnalysis;
using ModularPipelines.Attributes;
using ModularPipelines.Google.Options;
using ModularPipelines.Models;

namespace ModularPipelines.Google.Options;

/// <summary>
/// update a Dataplex task resource
/// </summary>
[ExcludeFromCodeCoverage]
[CliSubCommand("dataplex", "tasks", "update")]
public record GcloudDataplexTasksUpdateOptions : GcloudOptions
{
    /// <summary>
    /// Return immediately, without waiting for the operation in progress to     complete.
    /// </summary>
    [CliFlag("--async")]
    public bool? Async { get; set; }

    /// <summary>
    /// Description of the task.
    /// </summary>
    [CliOption("--description", Format = OptionFormat.EqualsSeparated)]
    public string? Description { get; set; }

    /// <summary>
    /// Display name of the task.
    /// </summary>
    [CliOption("--display-name", Format = OptionFormat.EqualsSeparated)]
    public string? DisplayName { get; set; }

    /// <summary>
    /// List of label KEY=VALUE pairs to update. If a label exists, its value     is modified. Otherwise, a new label is created.     Keys must start with a lowercase character and contain only hyphens     (-), underscores (_), lowercase characters, and numbers. Values must     contain only hyphens (-), underscores (_), lowercase characters, and     numbers.    At most one of these can be specified:     --clear-labels      Remove all labels. If --update-labels is also specified then      --clear-labels is applied first.      For example, to remove all labels:        $ gcloud dataplex tasks update --clear-labels      To remove all existing labels and create two new labels, foo and baz:        $ gcloud dataplex tasks update --clear-labels \         --update-labels foo=bar,baz=qux     --remove-labels=[KEY,...]      List of label keys to remove. If a label does not exist it is      silently ignored. If --update-labels is also specified then      --update-labels is applied first.    Spec related to how a task is executed.     --execution-args=[KEY=VALUE,...]      The arguments to pass to the task. The args can use placeholders of      the format ${placeholder} as part of key/value string. These will be      interpolated before passing the args to the driver. Currently      supported placeholders:      ▸ ${task_id}      ▸ ${job_time} To pass positional args, set the key as TASK_ARGS.       The value should be a comma-separated string of all the positional       arguments. To use a delimiter other than comma, refer to       https://cloud.google.com/sdk/gcloud/reference/topic/escaping. In       case of other keys being present in the args, then TASK_ARGS will       be passed as the last argument.     --execution-project=EXECUTION_PROJECT      The project in which jobs are run. By default, the project containing      the Lake is used. If a project is provided, the      --execution-service-account must belong to this same project.     --execution-service-account=EXECUTION_SERVICE_ACCOUNT      Service account to use to execute a task. If not provided, the      default Compute service account for the project is used.     --kms-key=KMS_KEY      The Cloud KMS key to use for encryption, of the form:      projects/{project_number}/locations/{location_id}/keyRings/{key-ring-name}/cryptoKeys/{key-name}     --max-job-execution-lifetime=MAX_JOB_EXECUTION_LIFETIME      The maximum duration before the job execution expires.    Select which task you want to schedule and provide the required   arguments:-    ◆ spark tasks    ◆ notebook tasks    At most one of these can be specified:     Config related to running custom Notebook tasks.      --notebook=NOTEBOOK       Google Cloud Storage URIs of the notebook file or the path to a       Notebook Content. Path to input notebook.      --notebook-archive-uris=[NOTEBOOK_ARCHIVE_URIS,...]       Google Cloud Storage URIs of archives to be extracted into the       working directory of each executor. Supported file types: .jar,       .tar, .tar.gz, .tgz, and .zip.      --notebook-file-uris=[NOTEBOOK_FILE_URIS,...]       Google Cloud Storage URIs of files to be placed in the working       directory of each executor.      Compute resources needed for a Task when using Dataproc Serverless.       --notebook-batch-executors-count=NOTEBOOK_BATCH_EXECUTORS_COUNT        Total number of job executors.       --notebook-batch-max-executors-count=NOTEBOOK_BATCH_MAX_EXECUTORS_COUNT        Max configurable executors. If max_executors_count &gt;        executors_count, then auto-scaling is enabled.      Container Image Runtime Configuration.       --notebook-container-image=NOTEBOOK_CONTAINER_IMAGE        Optional custom container image for the job.       --notebook-container-image-java-jars=[NOTEBOOK_CONTAINER_IMAGE_JAVA_JARS,...]        A list of Java JARS to add to the classpath. Valid input includes        Cloud Storage URIs to Jar binaries. For example,        gs://bucket-name/my/path/to/file.jar       --notebook-container-image-properties=[KEY=VALUE,...]        Override to common configuration of open source components        installed on the Dataproc cluster. The properties to set on        daemon config files. Property keys are specified in        prefix:property format, for example core:hadoop.tmp.dir. For more        information, see Cluster properties        (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).      Cloud VPC Network used to run the infrastructure.       --notebook-vpc-network-tags=[NOTEBOOK_VPC_NETWORK_TAGS,...]        List of network tags to apply to the job.       The Cloud VPC network identifier.       At most one of these can be specified:        --notebook-vpc-network-name=NOTEBOOK_VPC_NETWORK_NAME         The Cloud VPC network in which the job is run. By default, the         Cloud VPC network named Default within the project is used.        --notebook-vpc-sub-network-name=NOTEBOOK_VPC_SUB_NETWORK_NAME         The Cloud VPC sub-network in which the job is run.     Config related to running custom Spark tasks.      --spark-archive-uris=[SPARK_ARCHIVE_URIS,...]       Google Cloud Storage URIs of archives to be extracted into the       working directory of each executor. Supported file types: .jar,       .tar, .tar.gz, .tgz, and .zip.      --spark-file-uris=[SPARK_FILE_URIS,...]       Google Cloud Storage URIs of files to be placed in the working       directory of each executor.      Compute resources needed for a Task when using Dataproc Serverless.       --batch-executors-count=BATCH_EXECUTORS_COUNT        Total number of job executors.       --batch-max-executors-count=BATCH_MAX_EXECUTORS_COUNT        Max configurable executors. If max_executors_count &gt;        executors_count, then auto-scaling is enabled.      Container Image Runtime Configuration.       --container-image=CONTAINER_IMAGE        Optional custom container image for the job.       --container-image-java-jars=[CONTAINER_IMAGE_JAVA_JARS,...]        A list of Java JARS to add to the classpath. Valid input includes        Cloud Storage URIs to Jar binaries. For example,        gs://bucket-name/my/path/to/file.jar       --container-image-properties=[KEY=VALUE,...]        Override to common configuration of open source components        installed on the Dataproc cluster. The properties to set on        daemon config files. Property keys are specified in        prefix:property format, for example core:hadoop.tmp.dir. For more        information, see Cluster properties        (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).       --container-image-python-packages=[CONTAINER_IMAGE_PYTHON_PACKAGES,...]        A list of python packages to be installed. Valid formats include        Cloud Storage URI to a PIP installable library. For example,        gs://bucket-name/my/path/to/lib.tar.gz      Cloud VPC Network used to run the infrastructure.       --vpc-network-tags=[VPC_NETWORK_TAGS,...]        List of network tags to apply to the job.       The Cloud VPC network identifier.        --vpc-network-name=VPC_NETWORK_NAME         The Cloud VPC network in which the job is run. By default, the         Cloud VPC network named Default within the project is used.        --vpc-sub-network-name=VPC_SUB_NETWORK_NAME         The Cloud VPC sub-network in which the job is run.      The specification of the main method to call to drive the job. Specify     either the jar file that contains the main class or the main class     name.      At most one of these can be specified:       --spark-main-class=SPARK_MAIN_CLASS        The name of the driver's main class. The jar file that contains        the class must be in the default CLASSPATH or specified in       --spark-main-jar-file-uri=SPARK_MAIN_JAR_FILE_URI        The Google Cloud Storage URI of the jar file that contains the        main class. The execution args are passed in as a sequence of        named process arguments (--key=value).       --spark-python-script-file=SPARK_PYTHON_SCRIPT_FILE        The Google Cloud Storage URI of the main Python file to use as        the driver. Must be a .py file.       --spark-sql-script=SPARK_SQL_SCRIPT        The SQL query text.       --spark-sql-script-file=SPARK_SQL_SCRIPT_FILE        A reference to a query file. This can be the Google Cloud Storage        URI of the query file or it can the path to a SqlScript Content.    Spec related to how often and when a task should be triggered.     --trigger-disabled      Prevent the task from executing. This does not cancel already running      tasks. It is intended to temporarily disable RECURRING tasks.     --trigger-max-retires=TRIGGER_MAX_RETIRES      Number of retry attempts before aborting. Set to zero to never      attempt to retry a failed task.     --trigger-schedule=TRIGGER_SCHEDULE      Cron schedule (https://en.wikipedia.org/wiki/Cron) for running tasks      periodically.     --trigger-start-time=TRIGGER_START_TIME      The first run of the task will be after this time. If not specified,      the task will run shortly after being submitted if ON_DEMAND and      based on the schedule if RECURRING.
    /// </summary>
    [CliOption("--update-labels", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? UpdateLabels { get; set; }

}
