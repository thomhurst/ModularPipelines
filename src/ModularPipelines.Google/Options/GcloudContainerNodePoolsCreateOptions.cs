// <auto-generated>
// This file was generated by ModularPipelines.OptionsGenerator on 2025-12-28.
// Source: https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create
// Do not edit this file manually.
// </auto-generated>

#nullable enable

using System.Diagnostics.CodeAnalysis;
using ModularPipelines.Attributes;
using ModularPipelines.Google.Options;
using ModularPipelines.Models;
using ModularPipelines.Google.Enums;

namespace ModularPipelines.Google.Options;

/// <summary>
/// create a node pool in a running     cluster
/// </summary>
[ExcludeFromCodeCoverage]
[CliSubCommand("container", "node-pools", "create")]
public record GcloudContainerNodePoolsCreateOptions(
    [property: CliArgument(0, Name = "NAME")] string Name
) : GcloudOptions
{
    /// <summary>
    /// Attaches accelerators (e.g. GPUs) to all nodes.      type       (Required) The specific type (e.g. nvidia-tesla-t4 for NVIDIA T4)       of accelerator to attach to the instances. Use gcloud compute       accelerator-types list to learn about all available accelerator       types.      count       (Optional) The number of accelerators to attach to the instances.       The default value is 1.      gpu-driver-version       (Optional) The NVIDIA driver version to install. GPU_DRIVER_VERSION       must be one of:         `default`: Install the default driver version for this GKE version. For GKE version 1.30.1-gke.1156000 and later, this is the default option.         `latest`: Install the latest driver version available for this GKE version.         Can only be used for nodes that use Container-Optimized OS.         `disabled`: Skip automatic driver installation. You must manually install a         driver after you create the cluster. For GKE version 1.30.1-gke.1156000 and earlier, this is the default option.         To manually install the GPU driver, refer to https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers.      gpu-partition-size       (Optional) The GPU partition size used when running multi-instance       GPUs. For information about multi-instance GPUs, refer to:       https://cloud.google.com/kubernetes-engine/docs/how-to/gpus-multi      gpu-sharing-strategy       (Optional) The GPU sharing strategy (e.g. time-sharing) to use. For       information about GPU sharing, refer to:       https://cloud.google.com/kubernetes-engine/docs/concepts/timesharing-gpus      max-shared-clients-per-gpu       (Optional) The max number of containers allowed to share each GPU       on the node. This field is used together with gpu-sharing-strategy.
    /// </summary>
    [CliOption("--accelerator", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? Accelerator { get; set; }

    /// <summary>
    /// Attach an additional network interface to each node in the pool. This     parameter can be specified up to 7 times.     e.g. --additional-node-network network=dataplane,subnetwork=subnet-dp      network       (Required) The network to attach the new interface to.      subnetwork       (Required) The subnetwork to attach the new interface to.
    /// </summary>
    [CliOption("--additional-node-network", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? AdditionalNodeNetwork { get; set; }

    /// <summary>
    /// Specify the details of a secondary range to be used for an additional     pod network. Not needed if you use "host" typed NIC from this network.     This parameter can be specified up to 35 times.     e.g. --additional-pod-network     subnetwork=subnet-dp,pod-ipv4-range=sec-range-blue,max-pods-per-node=8.      subnetwork       (Optional) The name of the subnetwork to link the pod network to.       If not specified, the pod network defaults to the subnet connected       to the default network interface.      pod-ipv4-range       (Required) The name of the secondary range in the subnetwork. The       range must hold at least (2 * MAX_PODS_PER_NODE *       MAX_NODES_IN_RANGE) IPs.      max-pods-per-node       (Optional) Maximum amount of pods per node that can utilize this       ipv4-range. Defaults to NodePool (if specified) or Cluster value.
    /// </summary>
    [CliOption("--additional-pod-network", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? AdditionalPodNetwork { get; set; }

    /// <summary>
    /// Return immediately, without waiting for the operation in progress to     complete.
    /// </summary>
    [CliFlag("--async")]
    public bool? Async { get; set; }

    /// <summary>
    /// Autoscaled rollout policy options for blue-green upgrade.      wait-for-drain-duration       (Optional) Time in seconds to wait after cordoning the blue pool       before draining the nodes.       Examples:         $ gcloud container node-pools create node-pool-1 \           --cluster=example-cluster --enable-blue-green-upgrade \           --autoscaled-rollout-policy=""         $ gcloud container node-pools create node-pool-1 \           --cluster=example-cluster --enable-blue-green-upgrade \           --autoscaled-rollout-policy=wait-for-drain-duration=7200s
    /// </summary>
    [CliOption("--autoscaled-rollout-policy", Format = OptionFormat.EqualsSeparated)]
    public string? AutoscaledRolloutPolicy { get; set; }

    /// <summary>
    /// The Customer Managed Encryption Key used to encrypt the boot disk     attached to each node in the node pool. This should be of the form     projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME].     For more information about protecting resources with Cloud KMS Keys     please see:     https://cloud.google.com/compute/docs/disks/customer-managed-encryption
    /// </summary>
    [CliOption("--boot-disk-kms-key", Format = OptionFormat.EqualsSeparated)]
    public string? BootDiskKmsKey { get; set; }

    /// <summary>
    /// Configure the Provisioned IOPS for the node pool boot disks. Only valid     for hyperdisk-balanced boot disks.
    /// </summary>
    [CliOption("--boot-disk-provisioned-iops", Format = OptionFormat.EqualsSeparated)]
    public int? BootDiskProvisionedIops { get; set; }

    /// <summary>
    /// Configure the Provisioned Throughput for the node pool boot disks. Only     valid for hyperdisk-balanced boot disks.
    /// </summary>
    [CliOption("--boot-disk-provisioned-throughput", Format = OptionFormat.EqualsSeparated)]
    public string? BootDiskProvisionedThroughput { get; set; }

    /// <summary>
    /// The cluster to add the node pool to. Overrides the default     container/cluster property value for this command invocation.
    /// </summary>
    [CliOption("--cluster", Format = OptionFormat.EqualsSeparated)]
    public string? Cluster { get; set; }

    /// <summary>
    /// Enable confidential nodes for the node pool. Enabling Confidential     Nodes will create nodes using Confidential VM     https://cloud.google.com/compute/confidential-vm/docs/about-cvm.     CONFIDENTIAL_NODE_TYPE must be one of: sev, sev_snp, tdx, disabled.
    /// </summary>
    [CliOption("--confidential-node-type", Format = OptionFormat.EqualsSeparated)]
    public GcloudConfidentialNodeType? ConfidentialNodeType { get; set; }

    /// <summary>
    /// Path of the YAML file that contains containerd configuration entries     like configuring access to private image registries.     For detailed information on the configuration usage, please refer to     https://cloud.google.com/kubernetes-engine/docs/how-to/customize-containerd-configuration.     Note: Updating the containerd configuration of an existing cluster or     node pool requires recreation of the existing nodes, which might cause     disruptions in running workloads.     Use a full or relative path to a local file containing the value of     containerd_config.
    /// </summary>
    [CliOption("--containerd-config-from-file", Format = OptionFormat.EqualsSeparated)]
    public string? ContainerdConfigFromFile { get; set; }

    /// <summary>
    /// Specifies the number of local SSDs to be utilized for GKE Data Cache in     the node pool.
    /// </summary>
    [CliOption("--data-cache-count", Format = OptionFormat.EqualsSeparated)]
    public int? DataCacheCount { get; set; }

    /// <summary>
    /// Size for node VM boot disks in GB. Defaults to 100GB.
    /// </summary>
    [CliOption("--disk-size", Format = OptionFormat.EqualsSeparated)]
    public int? DiskSize { get; set; }

    /// <summary>
    /// Type of the node VM boot disk. For version 1.24 and later, defaults to     pd-balanced. For versions earlier than 1.24, defaults to pd-standard.     DISK_TYPE must be one of: pd-standard, pd-ssd, pd-balanced,     hyperdisk-balanced, hyperdisk-extreme, hyperdisk-throughput.
    /// </summary>
    [CliOption("--disk-type", Format = OptionFormat.EqualsSeparated)]
    public GcloudDiskType? DiskType { get; set; }

    /// <summary>
    /// Enables Cluster Autoscaler to treat the node pool as if it was     autoprovisioned.     Cluster Autoscaler will be able to delete the node pool if it's     unneeded.
    /// </summary>
    [CliFlag("--enable-autoprovisioning")]
    public bool? EnableAutoprovisioning { get; set; }

    /// <summary>
    /// Enable node autorepair feature for a node pool.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --enable-autorepair     Node autorepair is enabled by default for node pools using COS,     COS_CONTAINERD, UBUNTU or UBUNTU_CONTAINERD as a base image, use     --no-enable-autorepair to disable.     See     https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair     for more info.
    /// </summary>
    [CliFlag("--enable-autorepair")]
    public bool? EnableAutorepair { get; set; }

    /// <summary>
    /// Sets autoupgrade feature for a node pool.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --enable-autoupgrade     See https://cloud.google.com/kubernetes-engine/docs/node-auto-upgrades     for more info.     Enabled by default, use --no-enable-autoupgrade to disable.
    /// </summary>
    [CliFlag("--enable-autoupgrade")]
    public bool? EnableAutoupgrade { get; set; }

    /// <summary>
    /// Changes node pool upgrade strategy to blue-green upgrade.
    /// </summary>
    [CliFlag("--enable-blue-green-upgrade")]
    public bool? EnableBlueGreenUpgrade { get; set; }

    /// <summary>
    /// Enable confidential nodes for the node pool. Enabling Confidential     Nodes will create nodes using Confidential VM     https://cloud.google.com/compute/confidential-vm/docs/about-cvm.
    /// </summary>
    [CliFlag("--enable-confidential-nodes")]
    public bool? EnableConfidentialNodes { get; set; }

    /// <summary>
    /// Enable confidential storage for the node pool. Enabling Confidential     Storage will create boot disk with confidential mode
    /// </summary>
    [CliFlag("--enable-confidential-storage")]
    public bool? EnableConfidentialStorage { get; set; }

    /// <summary>
    /// Enable the use of GVNIC for this cluster. Requires re-creation of nodes     using either a node-pool upgrade or node-pool creation.
    /// </summary>
    [CliFlag("--enable-gvnic")]
    public bool? EnableGvnic { get; set; }

    /// <summary>
    /// Specifies whether to enable image streaming on node pool.
    /// </summary>
    [CliFlag("--enable-image-streaming")]
    public bool? EnableImageStreaming { get; set; }

    /// <summary>
    /// Enables the Kubelet's insecure read only port.     To disable the readonly port on a cluster or node-pool set the flag to     --no-enable-insecure-kubelet-readonly-port.
    /// </summary>
    [CliFlag("--enable-insecure-kubelet-readonly-port")]
    public bool? EnableInsecureKubeletReadonlyPort { get; set; }

    /// <summary>
    /// Enforces that kernel modules are signed on all nodes in the node pool.     This setting overrides the cluster-level setting. For example, if the     cluster disables enforcement, you can enable enforcement only for a     specific node pool. When the policy is modified on an existing node     pool, nodes will be immediately recreated to use the new policy. Use     --no-enable-kernel-module-signature-enforcement to disable.     Examples:       $ gcloud container node-pools create node-pool-1 \         --enable-kernel-module-signature-enforcement
    /// </summary>
    [CliFlag("--enable-kernel-module-signature-enforcement")]
    public bool? EnableKernelModuleSignatureEnforcement { get; set; }

    /// <summary>
    /// Enables the use of nested virtualization on the node pool. Defaults to     false. Can only be enabled on UBUNTU_CONTAINERD base image or     COS_CONTAINERD base image with version 1.28.4-gke.1083000 and above.
    /// </summary>
    [CliFlag("--enable-nested-virtualization")]
    public bool? EnableNestedVirtualization { get; set; }

    /// <summary>
    /// Enables provisioning nodes with private IP addresses only.     The control plane still communicates with all nodes through private IP     addresses only, regardless of whether private nodes are enabled or     disabled.
    /// </summary>
    [CliFlag("--enable-private-nodes")]
    public bool? EnablePrivateNodes { get; set; }

    /// <summary>
    /// Mark the nodepool as Queued only. This means that all new nodes can be     obtained only through queuing via ProvisioningRequest API.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --enable-queued-provisioning       ... and other required parameters, for more details see:       https://cloud.google.com/kubernetes-engine/docs/how-to/provisioningrequest
    /// </summary>
    [CliFlag("--enable-queued-provisioning")]
    public bool? EnableQueuedProvisioning { get; set; }

    /// <summary>
    /// Changes node pool upgrade strategy to surge upgrade.
    /// </summary>
    [CliFlag("--enable-surge-upgrade")]
    public bool? EnableSurgeUpgrade { get; set; }

    /// <summary>
    /// Start the node pool with Flex Start provisioning model.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --flex-start       and other required parameters, for more details see:       https://cloud.google.com/kubernetes-engine/docs/how-to/provisioningrequest
    /// </summary>
    [CliFlag("--flex-start")]
    public bool? FlexStart { get; set; }

    /// <summary>
    /// The image type to use for the node pool. Defaults to server-specified.     Image Type specifies the base OS that the nodes in the node pool will     run on. If an image type is specified, that will be assigned to the     node pool and all future upgrades will use the specified image type. If     it is not specified the server will pick the default image type.     The default image type and the list of valid image types are available     using the following command.       $ gcloud container get-server-config
    /// </summary>
    [CliOption("--image-type", Format = OptionFormat.EqualsSeparated)]
    public string? ImageType { get; set; }

    /// <summary>
    /// Labels to apply to the Google Cloud resources of node pools in the     Kubernetes Engine cluster. These are unrelated to Kubernetes labels.     Warning: Updating this label will causes the node(s) to be recreated.     Examples:       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --labels=label1=value1,label2=value2
    /// </summary>
    [CliOption("--labels", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Labels { get; set; }

    /// <summary>
    /// Specifies the logging variant that will be deployed on all the nodes in     the node pool. If the node pool doesn't specify a logging variant, then     the logging variant specified for the cluster will be deployed on all     the nodes in the node pool. Valid logging variants are MAX_THROUGHPUT,     DEFAULT. LOGGING_VARIANT must be one of:      DEFAULT       'DEFAULT' variant requests minimal resources but may not guarantee       high throughput.     MAX_THROUGHPUT       'MAX_THROUGHPUT' variant requests more node resources and is able       to achieve logging throughput up to 10MB per sec.
    /// </summary>
    [CliOption("--logging-variant", Format = OptionFormat.EqualsSeparated)]
    public string? LoggingVariant { get; set; }

    [CliOption("--machine-type", Format = OptionFormat.EqualsSeparated)]
    public string? MachineType { get; set; }

    /// <summary>
    /// The max number of pods per node for this node pool.     This flag sets the maximum number of pods that can be run at the same     time on a node. This will override the value given with     --default-max-pods-per-node flag set at the cluster level.     Must be used in conjunction with '--enable-ip-alias'.
    /// </summary>
    [CliOption("--max-pods-per-node", Format = OptionFormat.EqualsSeparated)]
    public string? MaxPodsPerNode { get; set; }

    /// <summary>
    /// Limit the runtime of each node in the node pool to the specified     duration.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --max-run-duration=3600s
    /// </summary>
    [CliOption("--max-run-duration", Format = OptionFormat.EqualsSeparated)]
    public string? MaxRunDuration { get; set; }

    [CliOption("--max-surge-upgrade", Format = OptionFormat.EqualsSeparated)]
    public string? MaxSurgeUpgrade { get; set; }

    /// <summary>
    /// Number of nodes that can be unavailable at the same time on each     upgrade of the node pool.     Specifies the number of nodes that can be unavailable at the same time     during this node pool's upgrades. For example, running the following     command will result in having 3 nodes being upgraded in parallel (1 +     2), but keeping always at least 3 (5 - 2) available each time the node     pool is upgraded:       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --num-nodes=5  \         --max-surge-upgrade=1 --max-unavailable-upgrade=2     Must be used in conjunction with '--max-surge-upgrade'.
    /// </summary>
    [CliOption("--max-unavailable-upgrade", Format = OptionFormat.EqualsSeparated)]
    public string? MaxUnavailableUpgrade { get; set; }

    /// <summary>
    /// Compute Engine metadata to be made available to the guest operating     system running on nodes within the node pool.     Each metadata entry is a key/value pair separated by an equals sign.     Metadata keys must be unique and less than 128 bytes in length. Values     must be less than or equal to 32,768 bytes in length. The total size of     all keys and values must be less than 512 KB. Multiple arguments can be     passed to this flag. For example:     --metadata key-1=value-1,key-2=value-2,key-3=value-3     Additionally, the following keys are reserved for use by Kubernetes     Engine:     ◆ cluster-location     ◆ cluster-name     ◆ cluster-uid     ◆ configure-sh     ◆ enable-os-login     ◆ gci-update-strategy     ◆ gci-ensure-gke-docker     ◆ instance-template     ◆ kube-env     ◆ startup-script     ◆ user-data     Google Kubernetes Engine sets the following keys by default:     ◆ serial-port-logging-enable     See also Compute Engine's documentation     (https://cloud.google.com/compute/docs/storing-retrieving-metadata) on     storing and retrieving instance metadata.
    /// </summary>
    [CliOption("--metadata", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Metadata { get; set; }

    /// <summary>
    /// Same as --metadata except that the value for the entry will be read     from a local file.
    /// </summary>
    [CliOption("--metadata-from-file", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? MetadataFromFile { get; set; }

    /// <summary>
    /// When specified, the nodes for the new node pool will be scheduled on     host with specified CPU architecture or a newer one.     Examples:       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --min-cpu-platform=PLATFORM     To list available CPU platforms in given zone, run:       $ gcloud beta compute zones describe ZONE \         --format="value(availableCpuPlatforms)"     CPU platform selection is available only in selected zones.
    /// </summary>
    [CliOption("--min-cpu-platform", Format = OptionFormat.EqualsSeparated)]
    public string? MinCpuPlatform { get; set; }

    /// <summary>
    /// Configures network performance settings for the node pool. If this flag     is not specified, the pool will be created with its default network     performance configuration.      total-egress-bandwidth-tier       Total egress bandwidth is the available outbound bandwidth from a       VM, regardless of whether the traffic is going to internal IP or       external IP destinations. The following tier values are allowed:       [TIER_UNSPECIFIED,TIER_1]
    /// </summary>
    [CliOption("--network-performance-configs", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? NetworkPerformanceConfigs { get; set; }

    /// <summary>
    /// Assign instances of this pool to run on the specified Google Compute     Engine node group. This is useful for running workloads on sole tenant     nodes.     To see available sole tenant node-groups, run:       $ gcloud compute sole-tenancy node-groups list     To create a sole tenant node group, run:       $ gcloud compute sole-tenancy node-groups create [GROUP_NAME]   \         --location [ZONE] --node-template [TEMPLATE_NAME]   \         --target-size [TARGET_SIZE]     See https://cloud.google.com/compute/docs/nodes for more information on     sole tenancy and node groups.
    /// </summary>
    [CliOption("--node-group", Format = OptionFormat.EqualsSeparated)]
    public string? NodeGroup { get; set; }

    /// <summary>
    /// Applies the given Kubernetes labels on all nodes in the new node pool.     Examples:       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster \         --node-labels=label1=value1,label2=value2     Updating the node pool's --node-labels flag applies the labels to the     Kubernetes Node objects for existing nodes in-place; it does not     re-create or replace nodes. New nodes, including ones created by     resizing or re-creating nodes, will have these labels on the Kubernetes     API Node object. The labels can be used in the nodeSelector field. See     https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/     for examples.     Note that Kubernetes labels, intended to associate cluster components     and resources with one another and manage resource lifecycles, are     different from Google Kubernetes Engine labels that are used for the     purpose of tracking billing and usage information.
    /// </summary>
    [CliOption("--node-labels", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? NodeLabels { get; set; }

    /// <summary>
    /// The set of zones in which the node pool's nodes should be located.     Multiple locations can be specified, separated by commas. For example:       $ gcloud container node-pools create node-pool-1 \         --cluster=sample-cluster \         --node-locations=us-central1-a,us-central1-b
    /// </summary>
    [CliOption("--node-locations", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? NodeLocations { get; set; }

    /// <summary>
    /// Time in seconds to be spent waiting during blue-green upgrade before     deleting the blue pool and completing the upgrade.       $ gcloud container node-pools create example-cluster \         --node-pool-soak-duration=600s
    /// </summary>
    [CliOption("--node-pool-soak-duration", Format = OptionFormat.EqualsSeparated)]
    public string? NodePoolSoakDuration { get; set; }

    /// <summary>
    /// Applies the given kubernetes taints on all nodes in the new node pool,     which can be used with tolerations for pod scheduling.     Examples:       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster \         --node-taints=key1=val1:NoSchedule,key2=val2:PreferNoSchedule     To read more about node-taints, see     https://cloud.google.com/kubernetes-engine/docs/node-taints.
    /// </summary>
    [CliOption("--node-taints", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? NodeTaints { get; set; }

    /// <summary>
    /// The Kubernetes version to use for nodes. Defaults to server-specified.     The default Kubernetes version is available using the following     command.       $ gcloud container get-server-config
    /// </summary>
    [CliOption("--node-version", Format = OptionFormat.EqualsSeparated)]
    public string? NodeVersion { get; set; }

    /// <summary>
    /// The number of nodes in the node pool in each of the cluster's zones.     Defaults to 3.     Exception: when --tpu-topology is specified for multi-host TPU machine     types the number of nodes will be defaulted to (product of topology)/(#     of chips per VM).
    /// </summary>
    [CliOption("--num-nodes", Format = OptionFormat.EqualsSeparated)]
    public string? NumNodes { get; set; }

    /// <summary>
    /// Opportunistic maintenance options.     node-idle-time: Time to be spent waiting for node to be idle before     starting maintenance, ending with 's'. Example: "3.5s"     window: The window of time that opportunistic maintenance can run,     ending with 's'. Example: A setting of 14 days (1209600s) implies that     opportunistic maintenance can only be ran in the 2 weeks leading up to     the scheduled maintenance date. Setting 28 days(2419200s) allows     opportunistic maintenance to run at any time in the scheduled     maintenance window.     min-nodes: Minimum number of nodes in the node pool to be available     during the opportunistic triggered maintenance.       $ gcloud container node-pools create example-cluster \         --opportunistic-maintenance=node-idle-time=600s,window=600s,\       min-nodes=2
    /// </summary>
    [CliOption("--opportunistic-maintenance", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? OpportunisticMaintenance { get; set; }

    /// <summary>
    /// Sets the Performance Monitoring Unit level. Valid values are     architectural, standard and enhanced. PERFORMANCE_MONITORING_UNIT must     be one of:      architectural       Enables architectural PMU events tied to non last level cache (LLC)       events.     enhanced       Enables most documented core/L2 and LLC PMU events.     standard       Enables most documented core/L2 PMU events.
    /// </summary>
    [CliOption("--performance-monitoring-unit", Format = OptionFormat.EqualsSeparated)]
    public string? PerformanceMonitoringUnit { get; set; }

    /// <summary>
    /// Indicates the desired resource policy to use.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --placement-policy my-placement
    /// </summary>
    [CliOption("--placement-policy", Format = OptionFormat.EqualsSeparated)]
    public string? PlacementPolicy { get; set; }

    /// <summary>
    /// Placement type allows to define the type of node placement within this     node pool.     UNSPECIFIED - No requirements on the placement of nodes. This is the     default option.     COMPACT - GKE will attempt to place the nodes in a close proximity to     each other. This helps to reduce the communication latency between the     nodes, but imposes additional limitations on the node pool size.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --placement-type=COMPACT     PLACEMENT_TYPE must be one of: UNSPECIFIED, COMPACT.
    /// </summary>
    [CliOption("--placement-type", Format = OptionFormat.EqualsSeparated)]
    public GcloudPlacementType? PlacementType { get; set; }

    /// <summary>
    /// Create nodes using preemptible VM instances in the new node pool.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --preemptible     New nodes, including ones created by resize or recreate, will use     preemptible VM instances. See     https://cloud.google.com/kubernetes-engine/docs/preemptible-vm for more     information on how to use Preemptible VMs with Kubernetes Engine.
    /// </summary>
    [CliFlag("--preemptible")]
    public bool? Preemptible { get; set; }

    /// <summary>
    /// Applies the specified comma-separated resource manager tags that has     the GCE_FIREWALL purpose to all nodes in the new node pool.     Examples:       $ gcloud container node-pools create example-node-pool \         --resource-manager-tags=tagKeys/1234=tagValues/2345       $ gcloud container node-pools create example-node-pool \         --resource-manager-tags=my-project/key1=value1       $ gcloud container node-pools create example-node-pool \         --resource-manager-tags=12345/key1=value1,23456/key2=value2       $ gcloud container node-pools create example-node-pool \         --resource-manager-tags=     All nodes, including nodes that are resized or re-created, will have     the specified tags on the corresponding Instance object in the Compute     Engine API. You can reference these tags in network firewall policy     rules. For instructions, see     https://cloud.google.com/firewall/docs/use-tags-for-firewalls.
    /// </summary>
    [CliOption("--resource-manager-tags", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? ResourceManagerTags { get; set; }

    /// <summary>
    /// Enables the requested sandbox on all nodes in the node pool.     Examples:       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --sandbox="type=gvisor"     The only supported type is 'gvisor'.
    /// </summary>
    [CliOption("--sandbox", Format = OptionFormat.EqualsSeparated)]
    public string? Sandbox { get; set; }

    /// <summary>
    /// Attaches secondary boot disks to all nodes.      disk-image       (Required) The full resource path to the source disk image to       create the secondary boot disks from.      mode       (Optional) The configuration mode for the secondary boot disks. The       default value is "CONTAINER_IMAGE_CACHE".
    /// </summary>
    [CliOption("--secondary-boot-disk", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? SecondaryBootDisk { get; set; }

    /// <summary>
    /// Enables monitoring and attestation of the boot integrity of the     instance. The attestation is performed against the integrity policy     baseline. This baseline is initially derived from the implicitly     trusted boot image when the instance is created.
    /// </summary>
    [CliFlag("--shielded-integrity-monitoring")]
    public bool? ShieldedIntegrityMonitoring { get; set; }

    /// <summary>
    /// The instance will boot with secure boot enabled.
    /// </summary>
    [CliFlag("--shielded-secure-boot")]
    public bool? ShieldedSecureBoot { get; set; }

    /// <summary>
    /// A integer value that specifies the minimum number of vCPUs that each     sole tenant node must have to use CPU overcommit. If not specified, the     CPU overcommit feature is disabled.
    /// </summary>
    [CliOption("--sole-tenant-min-node-cpus", Format = OptionFormat.EqualsSeparated)]
    public string? SoleTenantMinNodeCpus { get; set; }

    /// <summary>
    /// JSON/YAML file containing the configuration of desired sole tenant     nodes onto which this node pool could be backed by. These rules filter     the nodes according to their node affinity labels. A node's affinity     labels come from the node template of the group the node is in.     The file should contain a list of a JSON/YAML objects. For an example,     see     https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms#configure_node_affinity_labels.     The following list describes the fields:      key       Corresponds to the node affinity label keys of the Node resource.     operator       Specifies the node selection type. Must be one of: IN: Requires       Compute Engine to seek for matched nodes. NOT_IN: Requires Compute       Engine to avoid certain nodes.     values       Optional. A list of values which correspond to the node affinity       label values of the Node resource.
    /// </summary>
    [CliOption("--sole-tenant-node-affinity-file", Format = OptionFormat.EqualsSeparated)]
    public string? SoleTenantNodeAffinityFile { get; set; }

    /// <summary>
    /// Create nodes using spot VM instances in the new node pool.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --spot     New nodes, including ones created by resize or recreate, will use spot     VM instances.
    /// </summary>
    [CliFlag("--spot")]
    public bool? Spot { get; set; }

    /// <summary>
    /// Standard rollout policy options for blue-green upgrade.     Batch sizes are specified by one of, batch-node-count or batch-percent.     The duration between batches is specified by batch-soak-duration.       $ gcloud container node-pools create example-cluster \         --standard-rollout-policy=batch-node-count=3,\       batch-soak-duration=60s       $ gcloud container node-pools create example-cluster \         --standard-rollout-policy=batch-percent=0.3,\       batch-soak-duration=60s
    /// </summary>
    [CliOption("--standard-rollout-policy", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? StandardRolloutPolicy { get; set; }

    /// <summary>
    /// A list of storage pools where the node pool's boot disks will be     provisioned.     STORAGE_POOL must be in the format     projects/project/zones/zone/storagePools/storagePool
    /// </summary>
    [CliOption("--storage-pools", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? StoragePools { get; set; }

    /// <summary>
    /// Path of the YAML/JSON file that contains the node configuration,     including Linux kernel parameters (sysctls) and kubelet configs.     Examples:       kubeletConfig:        cpuManagerPolicy: static        memoryManager:         policy: Static        topologyManager:         policy: BestEffort         scope: pod       linuxConfig:        sysctl:         net.core.somaxconn: '2048'         net.ipv4.tcp_rmem: '4096 87380 6291456'        hugepageConfig:         hugepage_size2m: '1024'         hugepage_size1g: '2'        swapConfig:         enabled: true         bootDiskProfile:          swapSizeGib: 8        cgroupMode: 'CGROUP_MODE_V2'     List of supported kubelet configs in 'kubeletConfig'.      KEY                  VALUE      cpuManagerPolicy           either 'static' or 'none'      cpuCFSQuota              true or false (enabled by                         default)      cpuCFSQuotaPeriod           interval (e.g., '100ms'. The                         value must be between 1ms and 1                         second, inclusive.)      memoryManager             specify memory manager policy      topologyManager            specify topology manager policy                         and scope      podPidsLimit             integer (The value must be                         greater than or equal to 1024                         and less than 4194304.)      containerLogMaxSize          positive number plus unit suffix                         (e.g., '100Mi', '0.2Gi'. The                         value must be between 10Mi and                         500Mi, inclusive.)      containerLogMaxFiles         integer (The value must be                         between [2, 10].)      imageGcLowThresholdPercent      integer (The value must be                         between [10, 85], and lower than                         imageGcHighThresholdPercent.)      imageGcHighThresholdPercent      integer (The value must be                         between [10, 85], and greater                         than                         imageGcLowThresholdPercent.)      imageMinimumGcAge           interval (e.g., '100s', '1m'.                         The value must be less than                         '2m'.)      imageMaximumGcAge           interval (e.g., '100s', '1m'.                         The value must be greater than                         imageMinimumGcAge.)      evictionSoft             specify eviction soft thresholds      evictionSoftGracePeriod        specify eviction soft grace                         period      evictionMinimumReclaim        specify eviction minimum reclaim                         thresholds      evictionMaxPodGracePeriodSeconds   integer (Max grace period for                         pod termination during eviction,                         in seconds. The value must be                         between [0, 300].)      allowedUnsafeSysctls         list of sysctls (Allowlisted                         groups: 'kernel.shm*',                         'kernel.msg*', 'kernel.sem',                         'fs.mqueue.*', and 'net.*', and                         sysctls under the groups.)      singleProcessOomKill         true or false      maxParallelImagePulls         integer (The value must be                         between [2, 5].)     List of supported keys in memoryManager in 'kubeletConfig'.      KEY                     VALUE      policy                   either 'Static' or 'None'     List of supported keys in topologyManager in 'kubeletConfig'.      KEY                     VALUE      policy                   either 'none' or                            'best-effort' or                            'single-numa-node' or                            'restricted'      scope                    either 'pod' or                            'container'     List of supported keys in evictionSoft in 'kubeletConfig'.      KEY            VALUE      memoryAvailable      quantity (e.g., '100Mi', '1Gi'. Represents                   the amount of memory available before soft                   eviction. The value must be at least 100Mi                   and less than 50% of the node's memory.)      nodefsAvailable      percentage (e.g., '20%'. Represents the                   nodefs available before soft eviction. The                   value must be between 10% and 50%,                   inclusive.)      nodefsInodesFree      percentage (e.g., '20%'. Represents the                   nodefs inodes free before soft eviction.                   The value must be between 5% and 50%,                   inclusive.)      imagefsAvailable      percentage (e.g., '20%'. Represents the                   imagefs available before soft eviction. The                   value must be between 15% and 50%,                   inclusive.)      imagefsInodesFree     percentage (e.g., '20%'. Represents the                   imagefs inodes free before soft eviction.                   The value must be between 5% and 50%,                   inclusive.)      pidAvailable        percentage (e.g., '20%'. Represents the pid                   available before soft eviction. The value                   must be between 10% and 50%, inclusive.)     List of supported keys in evictionSoftGracePeriod in 'kubeletConfig'.      KEY            VALUE      memoryAvailable      duration (e.g., '30s', '1m'. The grace                   period for soft eviction for this resource.                   The value must be positive and no more than                   '5m'.)      nodefsAvailable      duration (e.g., '30s', '1m'. The grace                   period for soft eviction for this resource.                   The value must be positive and no more than                   '5m'.)      nodefsInodesFree      duration (e.g., '30s', '1m'. The grace                   period for soft eviction for this resource.                   The value must be positive and no more than                   '5m'.)      imagefsAvailable      duration (e.g., '30s', '1m'. The grace                   period for soft eviction for this resource.                   The value must be positive and no more than                   '5m'.)      imagefsInodesFree     duration (e.g., '30s', '1m'. The grace                   period for soft eviction for this resource.                   The value must be positive and no more than                   '5m'.)      pidAvailable        duration (e.g., '30s', '1m'. The grace                   period for soft eviction for this resource.                   The value must be positive and no more than                   '5m'.)     List of supported keys in evictionMinimumReclaim in 'kubeletConfig'.      KEY            VALUE      memoryAvailable      percentage (e.g., '5%'. Represents the                   minimum reclaim threshold for memory                   available. The value must be positive and                   no more than 10%.)      nodefsAvailable      percentage (e.g., '5%'. Represents the                   minimum reclaim threshold for nodefs                   available. The value must be positive and                   no more than 10%.)      nodefsInodesFree      percentage (e.g., '5%'. Represents the                   minimum reclaim threshold for nodefs inodes                   free. The value must be positive and no                   more than 10%.)      imagefsAvailable      percentage (e.g., '5%'. Represents the                   minimum reclaim threshold for imagefs                   available. The value must be positive and                   no more than 10%.)      imagefsInodesFree     percentage (e.g., '5%'. Represents the                   minimum reclaim threshold for imagefs                   inodes free. The value must be positive and                   no more than 10%.)      pidAvailable        percentage (e.g., '5%'. Represents the                   minimum reclaim threshold for pid                   available. The value must be positive and                   no more than 10%.)     List of supported sysctls in 'linuxConfig'.      KEY                         VALUE      net.core.netdev_max_backlog             Any positive                                integer, less than                                2147483647      net.core.rmem_default                Must be between                                [2304, 2147483647]      net.core.rmem_max                  Must be between                                [2304, 2147483647]      net.core.wmem_default                Must be between                                [4608, 2147483647]      net.core.wmem_max                  Must be between                                [4608, 2147483647]      net.core.optmem_max                 Any positive                                integer, less than                                2147483647      net.core.somaxconn                 Must be between                                [128, 2147483647]      net.ipv4.tcp_rmem                  Any positive                                integer tuple      net.ipv4.tcp_wmem                  Any positive                                integer tuple      net.ipv4.tcp_tw_reuse                Must be {0, 1, 2}      net.ipv4.tcp_mtu_probing              Must be {0, 1, 2}      net.ipv4.tcp_max_orphans              Must be between                                [16384, 262144]      net.ipv4.tcp_max_tw_buckets             Must be between                                [4096, 2147483647]      net.ipv4.tcp_syn_retries              Must be between                                [1, 127]      net.ipv4.tcp_ecn                  Must be {0, 1, 2}      net.ipv4.tcp_congestion_control           Supported values                                for COS: 'reno',                                'cubic', 'bbr',                                'lp', 'htcp'.                                Supported values                                for Ubuntu:                                'reno', 'cubic',                                'bbr', 'lp',                                'htcp', 'vegas',                                'dctcp', 'bic',                                'cdg',                                'highspeed',                                'hybla',                                'illinois', 'nv',                                'scalable',                                'veno',                                'westwood',                                'yeah'.      net.netfilter.nf_conntrack_max           Must be between                                [65536, 4194304]      net.netfilter.nf_conntrack_buckets         Must be between                                [65536, 524288].                                Recommend setting:                                nf_conntrack_max =                                nf_conntrack_bucke                                ts * 4      net.netfilter.nf_conntrack_tcp_timeout_close_wait  Must be between                                [60, 3600]      net.netfilter.nf_conntrack_tcp_timeout_time_wait  Must be between                                [1, 600]      net.netfilter.nf_conntrack_tcp_timeout_established Must be between                                [600, 86400]      net.netfilter.nf_conntrack_acct           Must be {0, 1}      kernel.shmmni                    Must be between                                [4096, 32768]      kernel.shmmax                    Must be between                                [0,                                184467440736927743                                99]      kernel.shmall                    Must be between                                [0,                                184467440736927743                                99]      kernel.perf_event_paranoid             Must be {-1, 0, 1,                                2, 3}      kernel.sched_rt_runtime_us             Must be [-1,                                1000000]      kernel.softlockup_panic               Must be {0, 1}      kernel.yama.ptrace_scope              Must be {0, 1, 2,                                3}      kernel.kptr_restrict                Must be {0, 1, 2}      kernel.dmesg_restrict                Must be {0, 1}      kernel.sysrq                    Must be [0, 511]      fs.aio-max-nr                    Must be between                                [65536, 4194304]      fs.file-max                     Must be between                                [104857, 67108864]      fs.inotify.max_user_instances            Must be between                                [8192, 1048576]      fs.inotify.max_user_watches             Must be between                                [8192, 1048576]      fs.nr_open                     Must be between                                [1048576,                                2147483584]      vm.dirty_background_ratio              Must be between                                [1, 100]      vm.dirty_background_bytes              Must be between                                [0, 68719476736]      vm.dirty_expire_centisecs              Must be between                                [0, 6000]      vm.dirty_ratio                   Must be between                                [1, 100]      vm.dirty_bytes                   Must be between                                [0, 68719476736]      vm.dirty_writeback_centisecs            Must be between                                [0, 1000]      vm.max_map_count                  Must be between                                [65536,                                2147483647]      vm.overcommit_memory                Must be one of {0,                                1, 2}      vm.overcommit_ratio                 Must be between                                [0, 100]      vm.vfs_cache_pressure                Must be between                                [0, 100]      vm.swappiness                    Must be between                                [0, 200]      vm.watermark_scale_factor              Must be between                                [10, 3000]      vm.min_free_kbytes                 Must be between                                [67584, 1048576]     List of supported hugepage size in 'hugepageConfig'.      KEY        VALUE      hugepage_size2m  Number of 2M huge pages, any positive integer      hugepage_size1g  Number of 1G huge pages, any positive integer     List of supported keys in 'swapConfig' under 'linuxConfig'.      KEY                     VALUE      enabled                   boolean      encryptionConfig              specify encryption                            settings for the swap                            space      bootDiskProfile               specify swap on the node's                            boot disk      ephemeralLocalSsdProfile          specify swap on the local                            SSD shared with pod                            ephemeral storage      dedicatedLocalSsdProfile          specify swap on a new,                            separate local NVMe SSD                            exclusively for swap     List of supported keys in 'encryptionConfig' under 'swapConfig'.      KEY                     VALUE      disabled                  boolean     List of supported keys in 'bootDiskProfile' under 'swapConfig'.      KEY                     VALUE      swapSizeGib                 integer      swapSizePercent               integer     List of supported keys in 'ephemeralLocalSsdProfile' under     'swapConfig'.      KEY                     VALUE      swapSizeGib                 integer      swapSizePercent               integer     List of supported keys in 'dedicatedLocalSsdProfile' under     'swapConfig'.      KEY                     VALUE      diskCount                  integer     Allocated hugepage size should not exceed 60% of available memory on     the node. For example, c2d-highcpu-4 has 8GB memory, total allocated     hugepage of 2m and 1g should not exceed 8GB * 0.6 = 4.8GB.     1G hugepages are only available in following machine familes: c3, m2,     c2d, c3d, h3, m3, a2, a3, g2.     Supported values for 'cgroupMode' under 'linuxConfig'.     ◆ CGROUP_MODE_V1: Use cgroupv1 on the node pool.     ◆ CGROUP_MODE_V2: Use cgroupv2 on the node pool.     ◆ CGROUP_MODE_UNSPECIFIED: Use the default GKE cgroup configuration.     Supported values for 'transparentHugepageEnabled' under 'linuxConfig'     which controls transparent hugepage support for anonymous memory.     ◆ TRANSPARENT_HUGEPAGE_ENABLED_ALWAYS: Transparent hugepage is      enabled system wide.     ◆ TRANSPARENT_HUGEPAGE_ENABLED_MADVISE: Transparent hugepage is      enabled inside MADV_HUGEPAGE regions. This is the default kernel      configuration.     ◆ TRANSPARENT_HUGEPAGE_ENABLED_NEVER: Transparent hugepage is      disabled.     ◆ TRANSPARENT_HUGEPAGE_ENABLED_UNSPECIFIED: Default value. GKE will      not modify the kernel configuration.     Supported values for 'transparentHugepageDefrag' under 'linuxConfig'     which defines the transparent hugepage defrag configuration on the     node.     ◆ TRANSPARENT_HUGEPAGE_DEFRAG_ALWAYS: It means that an application      requesting THP will stall on allocation failure and directly reclaim      pages and compact memory in an effort to allocate a THP immediately.     ◆ TRANSPARENT_HUGEPAGE_DEFRAG_DEFER: It means that an application      will wake kswapd in the background to reclaim pages and wake      kcompactd to compact memory so that THP is available in the near      future. It is the responsibility of khugepaged to then install the      THP pages later.     ◆ TRANSPARENT_HUGEPAGE_DEFRAG_DEFER_WITH_MADVISE: It means that an      application will enter direct reclaim and compaction like always, but      only for regions that have used madvise(MADV_HUGEPAGE); all other      regions will wake kswapd in the background to reclaim pages and wake      kcompactd to compact memory so that THP is available in the near      future.     ◆ TRANSPARENT_HUGEPAGE_DEFRAG_MADVISE: It means that an application      will enter direct reclaim and compaction like always, but only for      regions that have used madvise(MADV_HUGEPAGE); all other regions will      wake kswapd in the background to reclaim pages and wake kcompactd to      compact memory so that THP is available in the near future.     ◆ TRANSPARENT_HUGEPAGE_DEFRAG_NEVER: It means that an application      will never enter direct reclaim or compaction.     ◆ TRANSPARENT_HUGEPAGE_DEFRAG_UNSPECIFIED: Default value. GKE will      not modify the kernel configuration.     Note, updating the system configuration of an existing node pool     requires recreation of the nodes which which might cause a disruption.     Use a full or relative path to a local file containing the value of     system_config.
    /// </summary>
    [CliOption("--system-config-from-file", Format = OptionFormat.EqualsSeparated)]
    public string? SystemConfigFromFile { get; set; }

    /// <summary>
    /// Applies the given Compute Engine tags (comma separated) on all nodes in     the new node-pool. Example:       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --tags=tag1,tag2     New nodes, including ones created by resize or recreate, will have     these tags on the Compute Engine API instance object and can be used in     firewall rules. See     https://cloud.google.com/sdk/gcloud/reference/compute/firewall-rules/create     for examples.
    /// </summary>
    [CliOption("--tags", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? Tags { get; set; }

    /// <summary>
    /// The number of visible threads per physical core for each node. To     disable simultaneous multithreading (SMT) set this to 1.
    /// </summary>
    [CliOption("--threads-per-core", Format = OptionFormat.EqualsSeparated)]
    public string? ThreadsPerCore { get; set; }

    /// <summary>
    /// The desired physical topology for the PodSlice.       $ gcloud container node-pools create node-pool-1 \         --cluster=example-cluster --tpu-topology
    /// </summary>
    [CliOption("--tpu-topology", Format = OptionFormat.EqualsSeparated)]
    public string? TpuTopology { get; set; }

    /// <summary>
    /// Specifies the Windows Server Image to use when creating a Windows node     pool. Valid variants can be "ltsc2019", "ltsc2022". It means using     LTSC2019 server image or LTSC2022 server image. If the node pool     doesn't specify a Windows Server Image Os version, then Ltsc2019 will     be the default one to use. WINDOWS_OS_VERSION must be one of: ltsc2019,     ltsc2022.
    /// </summary>
    [CliOption("--windows-os-version", Format = OptionFormat.EqualsSeparated)]
    public GcloudWindowsOsVersion? WindowsOsVersion { get; set; }

    /// <summary>
    /// Type of metadata server available to pods running in the node pool.     WORKLOAD_METADATA must be one of:      GCE_METADATA       Pods running in this node pool have access to the node's underlying       Compute Engine Metadata Server.     GKE_METADATA       Run the Kubernetes Engine Metadata Server on this node. The       Kubernetes Engine Metadata Server exposes a metadata API to       workloads that is compatible with the V1 Compute Metadata APIs       exposed by the Compute Engine and App Engine Metadata Servers. This       feature can only be enabled if Workload Identity is enabled at the       cluster level.    At most one of these can be specified:     --create-pod-ipv4-range=[KEY=VALUE,...]      Create a new pod range for the node pool. The name and range of the      pod range can be customized via optional name and range keys.      name specifies the name of the secondary range to be created.      range specifies the IP range for the new secondary range. This can      either be a netmask size (e.g. "/20") or a CIDR range (e.g.      "10.0.0.0/20"). If a netmask size is specified, the IP is      automatically taken from the free space in the cluster's network.      Must be used in VPC native clusters. Can not be used in conjunction      with the --pod-ipv4-range option.      Examples:      Create a new pod range with a default name and size.        $ gcloud container node-pools create --create-pod-ipv4-range ""      Create a new pod range named my-range with netmask of size 21.        $ gcloud container node-pools create \          --create-pod-ipv4-range name=my-range,range=/21      Create a new pod range with a default name with the primary range of      10.100.0.0/16.        $ gcloud container node-pools create \          --create-pod-ipv4-range range=10.100.0.0/16      Create a new pod range with the name my-range with a default range.        $ gcloud container node-pools create \          --create-pod-ipv4-range name=my-range      Must be used in VPC native clusters. Can not be used in conjunction      with the --pod-ipv4-range option.     --pod-ipv4-range=NAME      Set the pod range to be used as the source for pod IPs for the pods      in this node pool. NAME must be the name of an existing subnetwork      secondary range in the subnetwork for this cluster.      Must be used in VPC native clusters. Cannot be used with      --create-ipv4-pod-range.      Examples:      Specify a pod range called other-range        $ gcloud container node-pools create --pod-ipv4-range other-range    Cluster autoscaling     --enable-autoscaling      Enables autoscaling for a node pool.      Enables autoscaling in the node pool specified by --node-pool or the      default node pool if --node-pool is not provided. If not already,      --max-nodes or --total-max-nodes must also be set.     --location-policy=LOCATION_POLICY      Location policy specifies the algorithm used when scaling-up the node      pool.      ▸ BALANCED - Is a best effort policy that aims to balance the sizes       of available zones.      ▸ ANY - Instructs the cluster autoscaler to prioritize utilization       of unused reservations, and reduces preemption risk for Spot VMs.      LOCATION_POLICY must be one of: BALANCED, ANY.     --max-nodes=MAX_NODES      Maximum number of nodes per zone in the node pool.      Maximum number of nodes per zone to which the node pool specified by      --node-pool (or default node pool if unspecified) can scale. Ignored      unless --enable-autoscaling is also specified.     --min-nodes=MIN_NODES      Minimum number of nodes per zone in the node pool.      Minimum number of nodes per zone to which the node pool specified by      --node-pool (or default node pool if unspecified) can scale. Ignored      unless --enable-autoscaling is also specified.     --total-max-nodes=TOTAL_MAX_NODES      Maximum number of all nodes in the node pool.      Maximum number of all nodes to which the node pool specified by      --node-pool (or default node pool if unspecified) can scale. Ignored      unless --enable-autoscaling is also specified.     --total-min-nodes=TOTAL_MIN_NODES      Minimum number of all nodes in the node pool.      Minimum number of all nodes to which the node pool specified by      --node-pool (or default node pool if unspecified) can scale. Ignored      unless --enable-autoscaling is also specified.    Specifies minimum number of nodes to be created when best effort   provisioning enabled.     --enable-best-effort-provision      Enable best effort provision for nodes     --min-provision-nodes=MIN_PROVISION_NODES      Specifies the minimum number of nodes to be provisioned during      creation    At most one of these can be specified:     --ephemeral-storage-local-ssd[=[count=COUNT]]      Parameters for the ephemeral storage filesystem. If unspecified,      ephemeral storage is backed by the boot disk.      Examples:        $ gcloud container node-pools create node-pool-1 --cluster=example \          cluster --ephemeral-storage-local-ssd count=2      'count' specifies the number of local SSDs to use to back ephemeral      storage. Local SDDs use NVMe interfaces. For first- and      second-generation machine types, a nonzero count field is required      for local ssd to be configured. For third-generation machine types,      the count field is optional because the count is inferred from the      machine type.      See https://cloud.google.com/compute/docs/disks/local-ssd for more      information.     --local-nvme-ssd-block[=[count=COUNT]]      Adds the requested local SSDs on all nodes in default node pool(s) in      the new cluster.      Examples:        $ gcloud container node-pools create node-pool-1 --cluster=example \          cluster --local-nvme-ssd-block count=2      'count' must be between 1-8      New nodes, including ones created by resize or recreate, will have      these local SSDs.      For first- and second-generation machine types, a nonzero count field      is required for local ssd to be configured. For third-generation      machine types, the count field is optional because the count is      inferred from the machine type.      See https://cloud.google.com/compute/docs/disks/local-ssd for more      information.     --local-ssd-count=LOCAL_SSD_COUNT      The number of local SSD disks to provision on each node, formatted      and mounted in the filesystem.      Local SSDs have a fixed 375 GB capacity per device. The number of      disks that can be attached to an instance is limited by the maximum      number of disks available on a machine, which differs by compute      zone. See https://cloud.google.com/compute/docs/disks/local-ssd for      more information.    At most one of these can be specified:     --location=LOCATION      Compute zone or region (e.g. us-central1-a or us-central1) for the      cluster. Overrides the default compute/region or compute/zone value      for this command invocation. Prefer using this flag over the --region      or --zone flags.     --region=REGION      Compute region (e.g. us-central1) for a regional cluster. Overrides      the default compute/region property value for this command      invocation.     --zone=ZONE, -z ZONE      Compute zone (e.g. us-central1-a) for a zonal cluster. Overrides the      default compute/zone property value for this command invocation.    Specifies the reservation for the node pool.     --reservation=RESERVATION      The name of the reservation, required when      --reservation-affinity=specific.     --reservation-affinity=RESERVATION_AFFINITY      The type of the reservation for the node pool. RESERVATION_AFFINITY      must be one of: any, none, specific.    Options to specify the node identity.     Scopes options.      --scopes=[SCOPE,...]; default="gke-default"       Specifies scopes for the node instances.       Examples:         $ gcloud container node-pools create node-pool-1 \           --cluster=example-cluster \           --scopes=https://www.googleapis.com/auth/devstorage.read_only         $ gcloud container node-pools create node-pool-1 \           --cluster=example-cluster \           --scopes=bigquery,storage-rw,compute-ro       Multiple scopes can be specified, separated by commas. Various       scopes are automatically added based on feature usage. Such scopes       are not added if an equivalent scope already exists.       ▫ monitoring-write: always added to ensure metrics can be written       ▫ logging-write: added if Cloud Logging is enabled        (--enable-cloud-logging/--logging)       ▫ monitoring: added if Cloud Monitoring is enabled        (--enable-cloud-monitoring/--monitoring)       ▫ gke-default: added for Autopilot clusters that use the default        service account       ▫ cloud-platform: added for Autopilot clusters that use any other        service account       SCOPE can be either the full URI of the scope or an alias. Default       scopes are assigned to all instances. Available aliases are:        Alias         URI        bigquery        https://www.googleapis.com/auth/bigquery        cloud-platform     https://www.googleapis.com/auth/cloud-platform        cloud-source-repos   https://www.googleapis.com/auth/source.full_control        cloud-source-repos-ro https://www.googleapis.com/auth/source.read_only        compute-ro       https://www.googleapis.com/auth/compute.readonly        compute-rw       https://www.googleapis.com/auth/compute        datastore       https://www.googleapis.com/auth/datastore        default        https://www.googleapis.com/auth/devstorage.read_only                   https://www.googleapis.com/auth/logging.write                   https://www.googleapis.com/auth/monitoring.write                   https://www.googleapis.com/auth/pubsub                   https://www.googleapis.com/auth/service.management.readonly                   https://www.googleapis.com/auth/servicecontrol                   https://www.googleapis.com/auth/trace.append        gke-default      https://www.googleapis.com/auth/devstorage.read_only                   https://www.googleapis.com/auth/logging.write                   https://www.googleapis.com/auth/monitoring                   https://www.googleapis.com/auth/service.management.readonly                   https://www.googleapis.com/auth/servicecontrol                   https://www.googleapis.com/auth/trace.append        logging-write     https://www.googleapis.com/auth/logging.write        monitoring       https://www.googleapis.com/auth/monitoring        monitoring-read    https://www.googleapis.com/auth/monitoring.read        monitoring-write    https://www.googleapis.com/auth/monitoring.write        pubsub         https://www.googleapis.com/auth/pubsub        service-control    https://www.googleapis.com/auth/servicecontrol        service-management   https://www.googleapis.com/auth/service.management.readonly        sql (deprecated)    https://www.googleapis.com/auth/sqlservice        sql-admin       https://www.googleapis.com/auth/sqlservice.admin        storage-full      https://www.googleapis.com/auth/devstorage.full_control        storage-ro       https://www.googleapis.com/auth/devstorage.read_only        storage-rw       https://www.googleapis.com/auth/devstorage.read_write        taskqueue       https://www.googleapis.com/auth/taskqueue        trace         https://www.googleapis.com/auth/trace.append        userinfo-email     https://www.googleapis.com/auth/userinfo.email       DEPRECATION WARNING: https://www.googleapis.com/auth/sqlservice       account scope and sql alias do not provide SQL instance management       capabilities and have been deprecated. Please, use       https://www.googleapis.com/auth/sqlservice.admin or sql-admin to       manage your Google SQL Service instances.     --service-account=SERVICE_ACCOUNT      The Google Cloud Platform Service Account to be used by the node VMs.      If a service account is specified, the cloud-platform and      userinfo.email scopes are used. If no Service Account is specified,      the project default service account is used.
    /// </summary>
    [CliOption("--workload-metadata", Format = OptionFormat.EqualsSeparated)]
    public GcloudWorkloadMetadata? WorkloadMetadata { get; set; }

}
