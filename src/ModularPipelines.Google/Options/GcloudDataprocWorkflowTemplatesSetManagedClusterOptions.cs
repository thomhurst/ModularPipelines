// <auto-generated>
// This file was generated by ModularPipelines.OptionsGenerator on 2025-12-08.
// Source: https://cloud.google.com/sdk/gcloud/reference/dataproc/workflow-templates/set-managed-cluster
// Do not edit this file manually.
// </auto-generated>

#nullable enable

using System.Diagnostics.CodeAnalysis;
using ModularPipelines.Attributes;
using ModularPipelines.Google.Options;
using ModularPipelines.Models;
using ModularPipelines.Google.Enums;

namespace ModularPipelines.Google.Options;

/// <summary>
/// set a managed      cluster for the workflow template
/// </summary>
[ExcludeFromCodeCoverage]
[CliSubCommand("dataproc", "workflow-templates", "set-managed-cluster")]
public record GcloudDataprocWorkflowTemplatesSetManagedClusterOptions : GcloudOptions
{
    /// <summary>
    /// ID of the autoscaling policy or fully qualified identifier for the      autoscaling policy.        To set the autoscaling_policy attribute:      * provide the argument --autoscaling-policy on the command line.
    /// </summary>
    [CliOption("--autoscaling-policy", Format = OptionFormat.EqualsSeparated)]
    public string? AutoscalingPolicy { get; set; }

    /// <summary>
    /// The Google Cloud Storage bucket to use by default to stage job      dependencies, miscellaneous config files, and job driver console output      when using this cluster.
    /// </summary>
    [CliOption("--bucket", Format = OptionFormat.EqualsSeparated)]
    public string? Bucket { get; set; }

    /// <summary>
    /// The name of the managed dataproc cluster. If unspecified, the workflow      template ID will be used.
    /// </summary>
    [CliOption("--cluster-name", Format = OptionFormat.EqualsSeparated)]
    public string? ClusterName { get; set; }

    /// <summary>
    /// The type of cluster. TYPE must be one of: standard, single-node,      zero-scale.
    /// </summary>
    [CliOption("--cluster-type", Format = OptionFormat.EqualsSeparated)]
    public GcloudClusterType? ClusterType { get; set; }

    /// <summary>
    /// Enables Confidential VM. See      https://cloud.google.com/compute/confidential-vm/docs for more      information. Note that Confidential VM can only be enabled when the      machine types are N2D      (https://cloud.google.com/compute/docs/machine-types#n2d_machine_types)      and the image is SEV Compatible.
    /// </summary>
    [CliFlag("--confidential-compute")]
    public bool? ConfidentialCompute { get; set; }

    /// <summary>
    /// Specify the name of a Dataproc Metastore service to be used as an      external metastore in the format:      "projects/{project-id}/locations/{region}/services/{service-name}".
    /// </summary>
    [CliOption("--dataproc-metastore", Format = OptionFormat.EqualsSeparated)]
    public string? DataprocMetastore { get; set; }

    /// <summary>
    /// Enable access to the web UIs of selected components on the cluster      through the component gateway.
    /// </summary>
    [CliFlag("--enable-component-gateway")]
    public bool? EnableComponentGateway { get; set; }

    [CliOption("--initialization-action-timeout", Format = OptionFormat.EqualsSeparated)]
    public int? InitializationActionTimeout { get; set; }

    /// <summary>
    /// A list of Google Cloud Storage URIs of executables to run on each node      in the cluster.
    /// </summary>
    [CliOption("--initialization-actions", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? InitializationActions { get; set; }

    /// <summary>
    /// List of label KEY=VALUE pairs to add.        Keys must start with a lowercase character and contain only hyphens      (-), underscores (_), lowercase characters, and numbers. Values must      contain only hyphens (-), underscores (_), lowercase characters, and      numbers.
    /// </summary>
    [CliOption("--labels", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Labels { get; set; }

    /// <summary>
    /// Attaches accelerators, such as GPUs, to the master instance(s).        type        The specific type of accelerator to attach to the instances, such        as nvidia-tesla-t4 for NVIDIA T4. Use gcloud compute        accelerator-types list to display available accelerator types.        count        The number of accelerators to attach to each instance. The default        value is 1.
    /// </summary>
    [CliOption("--master-accelerator", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? MasterAccelerator { get; set; }

    /// <summary>
    /// Indicates the IOPS      (https://cloud.google.com/compute/docs/disks/hyperdisks#iops) to      provision for the disk. This sets the limit for disk I/O operations per      second. This is only supported if the bootdisk type is      hyperdisk-balanced      (https://cloud.google.com/compute/docs/disks/hyperdisks).
    /// </summary>
    [CliOption("--master-boot-disk-provisioned-iops", Format = OptionFormat.EqualsSeparated)]
    public int? MasterBootDiskProvisionedIops { get; set; }

    /// <summary>
    /// Indicates the throughput      (https://cloud.google.com/compute/docs/disks/hyperdisks#throughput) to      provision for the disk. This sets the limit for throughput in MiB per      second. This is only supported if the bootdisk type is      hyperdisk-balanced      (https://cloud.google.com/compute/docs/disks/hyperdisks).
    /// </summary>
    [CliOption("--master-boot-disk-provisioned-throughput", Format = OptionFormat.EqualsSeparated)]
    public string? MasterBootDiskProvisionedThroughput { get; set; }

    /// <summary>
    /// The size of the boot disk. The value must be a whole number followed by      a size unit of KB for kilobyte, MB for megabyte, GB for gigabyte, or TB      for terabyte. For example, 10GB will produce a 10 gigabyte disk. The      minimum boot disk size is 10 GB. Boot disk size must be a multiple of 1      GB.
    /// </summary>
    [CliOption("--master-boot-disk-size", Format = OptionFormat.EqualsSeparated)]
    public int? MasterBootDiskSize { get; set; }

    /// <summary>
    /// The type of the boot disk. The value must be pd-balanced, pd-ssd, or      pd-standard.
    /// </summary>
    [CliOption("--master-boot-disk-type", Format = OptionFormat.EqualsSeparated)]
    public string? MasterBootDiskType { get; set; }

    /// <summary>
    /// Interface to use to attach local SSDs to master node(s) in a cluster.
    /// </summary>
    [CliOption("--master-local-ssd-interface", Format = OptionFormat.EqualsSeparated)]
    public string? MasterLocalSsdInterface { get; set; }

    /// <summary>
    /// The type of machine to use for the master. Defaults to      server-specified.
    /// </summary>
    [CliOption("--master-machine-type", Format = OptionFormat.EqualsSeparated)]
    public string? MasterMachineType { get; set; }

    /// <summary>
    /// When specified, the VM is scheduled on the host with a specified CPU      architecture or a more recent CPU platform that's available in that      zone. To list available CPU platforms in a zone, run:          $ gcloud compute zones describe ZONE        CPU platform selection may not be available in a zone. Zones that      support CPU platform selection provide an availableCpuPlatforms field,      which contains the list of available CPU platforms in the zone (see      Availability of CPU platforms for more information).
    /// </summary>
    [CliOption("--master-min-cpu-platform", Format = OptionFormat.EqualsSeparated)]
    public string? MasterMinCpuPlatform { get; set; }

    /// <summary>
    /// Minimum fraction of secondary worker nodes required to create the      cluster. If it is not met, cluster creation will fail. Must be a      decimal value between 0 and 1. The number of required secondary workers      is calculated by ceil(min-secondary-worker-fraction *      num_secondary_workers). Defaults to 0.0001.
    /// </summary>
    [CliOption("--min-secondary-worker-fraction", Format = OptionFormat.EqualsSeparated)]
    public string? MinSecondaryWorkerFraction { get; set; }

    /// <summary>
    /// The name of the sole-tenant node group to create the cluster on. Can be      a short name ("node-group-name") or in the format      "projects/{project-id}/zones/{zone}/nodeGroups/{node-group-name}".
    /// </summary>
    [CliOption("--node-group", Format = OptionFormat.EqualsSeparated)]
    public string? NodeGroup { get; set; }

    /// <summary>
    /// The number of local SSDs to attach to the master in a cluster.
    /// </summary>
    [CliOption("--num-master-local-ssds", Format = OptionFormat.EqualsSeparated)]
    public string? NumMasterLocalSsds { get; set; }

    /// <summary>
    /// The number of master nodes in the cluster.         Number of Masters Cluster Mode       1         Standard       3         High Availability
    /// </summary>
    [CliOption("--num-masters", Format = OptionFormat.EqualsSeparated)]
    public string? NumMasters { get; set; }

    /// <summary>
    /// The number of local SSDs to attach to each preemptible worker in a      cluster.
    /// </summary>
    [CliOption("--num-secondary-worker-local-ssds", Format = OptionFormat.EqualsSeparated)]
    public string? NumSecondaryWorkerLocalSsds { get; set; }

    /// <summary>
    /// The number of local SSDs to attach to each worker in a cluster.
    /// </summary>
    [CliOption("--num-worker-local-ssds", Format = OptionFormat.EqualsSeparated)]
    public string? NumWorkerLocalSsds { get; set; }

    /// <summary>
    /// List of optional components to be installed on cluster machines.        The following page documents the optional components that can be      installed:      https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/optional-components.
    /// </summary>
    [CliOption("--optional-components", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? OptionalComponents { get; set; }

    /// <summary>
    /// The private IPv6 Google access type for the cluster.      PRIVATE_IPV6_GOOGLE_ACCESS_TYPE must be one of: inherit-subnetwork,      outbound, bidirectional.
    /// </summary>
    [CliOption("--private-ipv6-google-access-type", Format = OptionFormat.EqualsSeparated)]
    public GcloudPrivateIpv6GoogleAccessType? PrivateIpv6GoogleAccessType { get; set; }

    /// <summary>
    /// Specifies configuration properties for installed packages, such as      Hadoop and Spark.        Properties are mapped to configuration files by specifying a prefix,      such as "core:io.serializations". The following are supported prefixes      and their mappings:         Prefix       File          Purpose of file       capacity-scheduler capacity-scheduler.xml Hadoop YARN Capacity                             Scheduler configuration       core        core-site.xml      Hadoop general                             configuration       distcp       distcp-default.xml   Hadoop Distributed Copy                             configuration       hadoop-env     hadoop-env.sh      Hadoop specific                             environment variables       hdfs        hdfs-site.xml      Hadoop HDFS configuration       hive        hive-site.xml      Hive configuration       mapred       mapred-site.xml     Hadoop MapReduce                             configuration       mapred-env     mapred-env.sh      Hadoop MapReduce specific                             environment variables       pig         pig.properties     Pig configuration       spark        spark-defaults.conf   Spark configuration       spark-env      spark-env.sh      Spark specific environment                             variables       yarn        yarn-site.xml      Hadoop YARN configuration       yarn-env      yarn-env.sh       Hadoop YARN specific                             environment variables        See      https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties      for more information.
    /// </summary>
    [CliOption("--properties", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Properties { get; set; }

    /// <summary>
    /// Attaches accelerators, such as GPUs, to the secondary-worker      instance(s).        type        The specific type of accelerator to attach to the instances, such        as nvidia-tesla-t4 for NVIDIA T4. Use gcloud compute        accelerator-types list to display available accelerator types.        count        The number of accelerators to attach to each instance. The default        value is 1.
    /// </summary>
    [CliOption("--secondary-worker-accelerator", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? SecondaryWorkerAccelerator { get; set; }

    /// <summary>
    /// The size of the boot disk. The value must be a whole number followed by      a size unit of KB for kilobyte, MB for megabyte, GB for gigabyte, or TB      for terabyte. For example, 10GB will produce a 10 gigabyte disk. The      minimum boot disk size is 10 GB. Boot disk size must be a multiple of 1      GB.
    /// </summary>
    [CliOption("--secondary-worker-boot-disk-size", Format = OptionFormat.EqualsSeparated)]
    public int? SecondaryWorkerBootDiskSize { get; set; }

    /// <summary>
    /// The type of the boot disk. The value must be pd-balanced, pd-ssd, or      pd-standard.
    /// </summary>
    [CliOption("--secondary-worker-boot-disk-type", Format = OptionFormat.EqualsSeparated)]
    public string? SecondaryWorkerBootDiskType { get; set; }

    /// <summary>
    /// Interface to use to attach local SSDs to each secondary worker in a      cluster.
    /// </summary>
    [CliOption("--secondary-worker-local-ssd-interface", Format = OptionFormat.EqualsSeparated)]
    public string? SecondaryWorkerLocalSsdInterface { get; set; }

    /// <summary>
    /// Types of machines with optional rank for secondary workers to use.      Defaults to server-specified.eg.      --secondary-worker-machine-types="type=e2-standard-8,type=t2d-standard-8,rank=0"
    /// </summary>
    [CliOption("--secondary-worker-machine-types", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? SecondaryWorkerMachineTypes { get; set; }

    /// <summary>
    /// This flag sets the base number of Standard VMs to use for secondary      workers      (https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers).      Dataproc will create only standard VMs until it reaches this number,      then it will mix Spot and Standard VMs according to      SECONDARY_WORKER_STANDARD_CAPACITY_PERCENT_ABOVE_BASE.
    /// </summary>
    [CliOption("--secondary-worker-standard-capacity-base", Format = OptionFormat.EqualsSeparated)]
    public string? SecondaryWorkerStandardCapacityBase { get; set; }

    /// <summary>
    /// When combining Standard and Spot VMs for secondary-workers      (https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers)      once the number of Standard VMs specified by      SECONDARY_WORKER_STANDARD_CAPACITY_BASE has been used, this flag      specifies the percentage of the total number of additional Standard VMs      secondary workers will use. Spot VMs will be used for the remaining      percentage.
    /// </summary>
    [CliOption("--secondary-worker-standard-capacity-percent-above-base", Format = OptionFormat.EqualsSeparated)]
    public string? SecondaryWorkerStandardCapacityPercentAboveBase { get; set; }

    /// <summary>
    /// Enables monitoring and attestation of the boot integrity of the      cluster's VMs. vTPM (virtual Trusted Platform Module) must also be      enabled. A TPM is a hardware module that can be used for different      security operations, such as remote attestation, encryption, and      sealing of keys.
    /// </summary>
    [CliFlag("--shielded-integrity-monitoring")]
    public bool? ShieldedIntegrityMonitoring { get; set; }

    /// <summary>
    /// The cluster's VMs will boot with secure boot enabled.
    /// </summary>
    [CliFlag("--shielded-secure-boot")]
    public bool? ShieldedSecureBoot { get; set; }

    /// <summary>
    /// The cluster's VMs will boot with the TPM (Trusted Platform Module)      enabled. A TPM is a hardware module that can be used for different      security operations, such as remote attestation, encryption, and      sealing of keys.
    /// </summary>
    [CliFlag("--shielded-vtpm")]
    public bool? ShieldedVtpm { get; set; }

    /// <summary>
    /// The Google Cloud Storage bucket to use by default to store ephemeral      cluster and jobs data, such as Spark and MapReduce history files.
    /// </summary>
    [CliOption("--temp-bucket", Format = OptionFormat.EqualsSeparated)]
    public string? TempBucket { get; set; }

    /// <summary>
    /// Cluster tier. TIER must be one of: premium, standard.
    /// </summary>
    [CliOption("--tier", Format = OptionFormat.EqualsSeparated)]
    public GcloudTier? Tier { get; set; }

    /// <summary>
    /// Attaches accelerators, such as GPUs, to the worker instance(s).        type        The specific type of accelerator to attach to the instances, such        as nvidia-tesla-t4 for NVIDIA T4. Use gcloud compute        accelerator-types list to display available accelerator types.        count        The number of accelerators to attach to each instance. The default        value is 1.
    /// </summary>
    [CliOption("--worker-accelerator", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? WorkerAccelerator { get; set; }

    /// <summary>
    /// Indicates the IOPS      (https://cloud.google.com/compute/docs/disks/hyperdisks#iops) to      provision for the disk. This sets the limit for disk I/O operations per      second. This is only supported if the bootdisk type is      hyperdisk-balanced      (https://cloud.google.com/compute/docs/disks/hyperdisks).
    /// </summary>
    [CliOption("--worker-boot-disk-provisioned-iops", Format = OptionFormat.EqualsSeparated)]
    public int? WorkerBootDiskProvisionedIops { get; set; }

    /// <summary>
    /// Indicates the throughput      (https://cloud.google.com/compute/docs/disks/hyperdisks#throughput) to      provision for the disk. This sets the limit for throughput in MiB per      second. This is only supported if the bootdisk type is      hyperdisk-balanced      (https://cloud.google.com/compute/docs/disks/hyperdisks).
    /// </summary>
    [CliOption("--worker-boot-disk-provisioned-throughput", Format = OptionFormat.EqualsSeparated)]
    public string? WorkerBootDiskProvisionedThroughput { get; set; }

    /// <summary>
    /// The size of the boot disk. The value must be a whole number followed by      a size unit of KB for kilobyte, MB for megabyte, GB for gigabyte, or TB      for terabyte. For example, 10GB will produce a 10 gigabyte disk. The      minimum boot disk size is 10 GB. Boot disk size must be a multiple of 1      GB.
    /// </summary>
    [CliOption("--worker-boot-disk-size", Format = OptionFormat.EqualsSeparated)]
    public int? WorkerBootDiskSize { get; set; }

    /// <summary>
    /// The type of the boot disk. The value must be pd-balanced, pd-ssd, or      pd-standard.
    /// </summary>
    [CliOption("--worker-boot-disk-type", Format = OptionFormat.EqualsSeparated)]
    public string? WorkerBootDiskType { get; set; }

    /// <summary>
    /// Interface to use to attach local SSDs to each worker in a cluster.
    /// </summary>
    [CliOption("--worker-local-ssd-interface", Format = OptionFormat.EqualsSeparated)]
    public string? WorkerLocalSsdInterface { get; set; }

    /// <summary>
    /// When specified, the VM is scheduled on the host with a specified CPU      architecture or a more recent CPU platform that's available in that      zone. To list available CPU platforms in a zone, run:          $ gcloud compute zones describe ZONE        CPU platform selection may not be available in a zone. Zones that      support CPU platform selection provide an availableCpuPlatforms field,      which contains the list of available CPU platforms in the zone (see      Availability of CPU platforms for more information).
    /// </summary>
    [CliOption("--worker-min-cpu-platform", Format = OptionFormat.EqualsSeparated)]
    public string? WorkerMinCpuPlatform { get; set; }

    /// <summary>
    /// The compute zone (e.g. us-central1-a) for the cluster. If empty and      --region is set to a value other than global, the server will pick a      zone in the region. Overrides the default compute/zone property value      for this command invocation.      Specifying these flags will enable Secure Multi-Tenancy for the cluster.      At most one of these can be specified:       --identity-config-file=IDENTITY_CONFIG_FILE       Path to a YAML (or JSON) file containing the configuration for Secure       Multi-Tenancy on the cluster. The path can be a Cloud Storage URL       (Example: 'gs://path/to/file') or a local file system path. If you       pass "-" as the value of the flag the file content will be read from       stdin.         The YAML file is formatted as follows:            # Required. The mapping from user accounts to service accounts.          user_service_account_mapping:           bob@company.com: service-account-bob@project.iam.gserviceaccount.com           alice@company.com: service-account-alice@project.iam.gserviceaccount.com       --secure-multi-tenancy-user-mapping=SECURE_MULTI_TENANCY_USER_MAPPING       A string of user-to-service-account mappings. Mappings are separated       by commas, and each mapping takes the form of       "user-account:service-account". Example:       "bob@company.com:service-account-bob@project.iam.gserviceaccount.com,alice@company.com:service-account-alice@project.iam.gserviceaccount.com".      At most one of these can be specified:       --image=IMAGE       The custom image used to create the cluster. It can be the image       name, the image URI, or the image family URI, which selects the       latest image from the family.       --image-version=VERSION       The image version to use for the cluster. Defaults to the latest       version.      Specifying these flags will enable Kerberos for the cluster.      At most one of these can be specified:       --kerberos-config-file=KERBEROS_CONFIG_FILE       Path to a YAML (or JSON) file containing the configuration for       Kerberos on the cluster. If you pass - as the value of the flag the       file content will be read from stdin.         The YAML file is formatted as follows:            # Optional. Flag to indicate whether to Kerberize the cluster.          # The default value is true.          enable_kerberos: true            # Optional. The Google Cloud Storage URI of a KMS encrypted file          # containing the root principal password.          root_principal_password_uri: gs://bucket/password.encrypted            # Optional. The URI of the Cloud KMS key used to encrypt          # sensitive files.          kms_key_uri:           projects/myproject/locations/global/keyRings/mykeyring/cryptoKeys/my-key            # Configuration of SSL encryption. If specified, all sub-fields          # are required. Otherwise, Dataproc will provide a self-signed          # certificate and generate the passwords.          ssl:           # Optional. The Google Cloud Storage URI of the keystore file.           keystore_uri: gs://bucket/keystore.jks             # Optional. The Google Cloud Storage URI of a KMS encrypted           # file containing the password to the keystore.           keystore_password_uri: gs://bucket/keystore_password.encrypted             # Optional. The Google Cloud Storage URI of a KMS encrypted           # file containing the password to the user provided key.           key_password_uri: gs://bucket/key_password.encrypted             # Optional. The Google Cloud Storage URI of the truststore           # file.           truststore_uri: gs://bucket/truststore.jks             # Optional. The Google Cloud Storage URI of a KMS encrypted           # file containing the password to the user provided           # truststore.           truststore_password_uri:            gs://bucket/truststore_password.encrypted            # Configuration of cross realm trust.          cross_realm_trust:           # Optional. The remote realm the Dataproc on-cluster KDC will           # trust, should the user enable cross realm trust.           realm: REMOTE.REALM             # Optional. The KDC (IP or hostname) for the remote trusted           # realm in a cross realm trust relationship.           kdc: kdc.remote.realm             # Optional. The admin server (IP or hostname) for the remote           # trusted realm in a cross realm trust relationship.           admin_server: admin-server.remote.realm             # Optional. The Google Cloud Storage URI of a KMS encrypted           # file containing the shared password between the on-cluster           # Kerberos realm and the remote trusted realm, in a cross           # realm trust relationship.           shared_password_uri:            gs://bucket/cross-realm.password.encrypted            # Optional. The Google Cloud Storage URI of a KMS encrypted file          # containing the master key of the KDC database.          kdc_db_key_uri: gs://bucket/kdc_db_key.encrypted            # Optional. The lifetime of the ticket granting ticket, in          # hours. If not specified, or user specifies 0, then default          # value 10 will be used.          tgt_lifetime_hours: 1            # Optional. The name of the Kerberos realm. If not specified,          # the uppercased domain name of the cluster will be used.          realm: REALM.NAME       --enable-kerberos       Enable Kerberos on the cluster.       --kerberos-root-principal-password-uri=KERBEROS_ROOT_PRINCIPAL_PASSWORD_URI       Google Cloud Storage URI of a KMS encrypted file containing the root       principal password. Must be a Cloud Storage URL beginning with       'gs://'.       Key resource - The Cloud KMS (Key Management Service) cryptokey that     will be used to protect the password. The 'Compute Engine Service Agent'     service account must hold permission 'Cloud KMS CryptoKey     Encrypter/Decrypter'. The arguments in this group can be used to specify     the attributes of this resource.        --kerberos-kms-key=KERBEROS_KMS_KEY        ID of the key or fully qualified identifier for the key.          To set the kms-key attribute:        - provide the argument --kerberos-kms-key on the command line.          This flag argument must be specified if any of the other arguments        in this group are specified.        --kerberos-kms-key-keyring=KERBEROS_KMS_KEY_KEYRING        The KMS keyring of the key.          To set the kms-keyring attribute:        - provide the argument --kerberos-kms-key on the command line         with a fully specified name;        - provide the argument --kerberos-kms-key-keyring on the command         line.        --kerberos-kms-key-location=KERBEROS_KMS_KEY_LOCATION        The Google Cloud location for the key.          To set the kms-location attribute:        - provide the argument --kerberos-kms-key on the command line         with a fully specified name;        - provide the argument --kerberos-kms-key-location on the command         line.        --kerberos-kms-key-project=KERBEROS_KMS_KEY_PROJECT        The Google Cloud project for the key.          To set the kms-project attribute:        - provide the argument --kerberos-kms-key on the command line         with a fully specified name;        - provide the argument --kerberos-kms-key-project on the command         line;        - set the property core/project.      Key resource - The Cloud KMS (Key Management Service) cryptokey that will    be used to protect the cluster. The 'Compute Engine Service Agent' service    account must hold permission 'Cloud KMS CryptoKey Encrypter/Decrypter'.    The arguments in this group can be used to specify the attributes of this    resource.       --kms-key=KMS_KEY       ID of the key or fully qualified identifier for the key.         To set the kms-key attribute:       + provide the argument --kms-key on the command line.         This flag argument must be specified if any of the other arguments in       this group are specified.       --kms-keyring=KMS_KEYRING       The KMS keyring of the key.         To set the kms-keyring attribute:       + provide the argument --kms-key on the command line with a fully        specified name;       + provide the argument --kms-keyring on the command line.       --kms-location=KMS_LOCATION       The Google Cloud location for the key.         To set the kms-location attribute:       + provide the argument --kms-key on the command line with a fully        specified name;       + provide the argument --kms-location on the command line.       --kms-project=KMS_PROJECT       The Google Cloud project for the key.         To set the kms-project attribute:       + provide the argument --kms-key on the command line with a fully        specified name;       + provide the argument --kms-project on the command line;       + set the property core/project.      Compute Engine options for Dataproc clusters.       --metadata=KEY=VALUE,[KEY=VALUE,...]       Metadata to be made available to the guest operating system running       on the instances       --resource-manager-tags=KEY=VALUE,[KEY=VALUE,...]       Specifies a list of resource manager tags to apply to each cluster       node (master and worker nodes).       --scopes=SCOPE,[SCOPE,...]       Specifies scopes for the node instances. Multiple SCOPEs can be       specified, separated by commas. Examples:           $ gcloud dataproc workflow-templates set-managed-cluster \           example-cluster \           --scopes https://www.googleapis.com/auth/bigtable.admin           $ gcloud dataproc workflow-templates set-managed-cluster \           example-cluster --scopes sqlservice,bigquery         The following minimum scopes are necessary for the cluster to       function properly and are always added, even if not explicitly       specified:           https://www.googleapis.com/auth/devstorage.read_write         https://www.googleapis.com/auth/logging.write         If the --scopes flag is not specified, the following default scopes       are also included:           https://www.googleapis.com/auth/bigquery         https://www.googleapis.com/auth/bigtable.admin.table         https://www.googleapis.com/auth/bigtable.data         https://www.googleapis.com/auth/devstorage.full_control         If you want to enable all scopes use the 'cloud-platform' scope.         SCOPE can be either the full URI of the scope or an alias. Default       scopes are assigned to all instances. Available aliases are:          Alias         URI        bigquery        https://www.googleapis.com/auth/bigquery        cloud-platform     https://www.googleapis.com/auth/cloud-platform        cloud-source-repos   https://www.googleapis.com/auth/source.full_control        cloud-source-repos-ro https://www.googleapis.com/auth/source.read_only        compute-ro       https://www.googleapis.com/auth/compute.readonly        compute-rw       https://www.googleapis.com/auth/compute        datastore       https://www.googleapis.com/auth/datastore        default        https://www.googleapis.com/auth/devstorage.read_only                   https://www.googleapis.com/auth/logging.write                   https://www.googleapis.com/auth/monitoring.write                   https://www.googleapis.com/auth/pubsub                   https://www.googleapis.com/auth/service.management.readonly                   https://www.googleapis.com/auth/servicecontrol                   https://www.googleapis.com/auth/trace.append        gke-default      https://www.googleapis.com/auth/devstorage.read_only                   https://www.googleapis.com/auth/logging.write                   https://www.googleapis.com/auth/monitoring                   https://www.googleapis.com/auth/service.management.readonly                   https://www.googleapis.com/auth/servicecontrol                   https://www.googleapis.com/auth/trace.append        logging-write     https://www.googleapis.com/auth/logging.write        monitoring       https://www.googleapis.com/auth/monitoring        monitoring-read    https://www.googleapis.com/auth/monitoring.read        monitoring-write    https://www.googleapis.com/auth/monitoring.write        pubsub         https://www.googleapis.com/auth/pubsub        service-control    https://www.googleapis.com/auth/servicecontrol        service-management   https://www.googleapis.com/auth/service.management.readonly        sql (deprecated)    https://www.googleapis.com/auth/sqlservice        sql-admin       https://www.googleapis.com/auth/sqlservice.admin        storage-full      https://www.googleapis.com/auth/devstorage.full_control        storage-ro       https://www.googleapis.com/auth/devstorage.read_only        storage-rw       https://www.googleapis.com/auth/devstorage.read_write        taskqueue       https://www.googleapis.com/auth/taskqueue        trace         https://www.googleapis.com/auth/trace.append        userinfo-email     https://www.googleapis.com/auth/userinfo.email         DEPRECATION WARNING: https://www.googleapis.com/auth/sqlservice       account scope and sql alias do not provide SQL instance management       capabilities and have been deprecated. Please, use       https://www.googleapis.com/auth/sqlservice.admin or sql-admin to       manage your Google SQL Service instances.       --service-account=SERVICE_ACCOUNT       The Google Cloud IAM service account to be authenticated as.       --tags=TAG,[TAG,...]       Specifies a list of tags to apply to the instance. These tags allow       network firewall rules and routes to be applied to specified VM       instances. See gcloud compute firewall-rules create(1) for more       details.         To read more about configuring network tags, read this guide:       https://cloud.google.com/vpc/docs/add-remove-network-tags         To list instances with their respective status and tags, run:           $ gcloud compute instances list \           --format='table(name,status,tags.list())'         To list instances tagged with a specific tag, tag1, run:           $ gcloud compute instances list --filter='tags:tag1'       At most one of these can be specified:        --network=NETWORK        The Compute Engine network that the VM instances of the cluster        will be part of. This is mutually exclusive with --subnet. If        neither is specified, this defaults to the "default" network.        --subnet=SUBNET        Specifies the subnet that the cluster will be part of. This is        mutally exclusive with --network.       Specifies the reservation for the instance.        --reservation=RESERVATION        The name of the reservation, required when        --reservation-affinity=specific.        --reservation-affinity=RESERVATION_AFFINITY; default="any"        The type of reservation for the instance. RESERVATION_AFFINITY must        be one of: any, none, specific.
    /// </summary>
    [CliOption("--zone", Format = OptionFormat.EqualsSeparated)]
    public string? Zone { get; set; }

    /// <summary>
    /// Specifies a list of cluster Metric Sources      (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics)      to collect custom metrics. METRIC_SOURCE must be one of: FLINK, HDFS,      HIVEMETASTORE, HIVESERVER2, MONITORING_AGENT_DEFAULTS, SPARK,      SPARK_HISTORY_SERVER, YARN.      At most one of these can be specified:       --metric-overrides=[METRIC_SOURCE:INSTANCE:GROUP:METRIC,...]       List of metrics that override the default metrics enabled for the       metric sources. Any of the available OSS metrics       (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics)       and all Spark metrics, can be listed for collection as a metric       override. Override metric values are case sensitive, and must be       provided, if appropriate, in CamelCase format, for example:         sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed       hiveserver2:JVM:Memory:NonHeapMemoryUsage.used         Only the specified overridden metrics will be collected from a given       metric source. For example, if one or more spark:executive metrics       are listed as metric overrides, other SPARK metrics will not be       collected. The collection of default OSS metrics from other metric       sources is unaffected. For example, if both SPARK and YARN metric       sources are enabled, and overrides are provided for Spark metrics       only, all default YARN metrics will be collected.         The source of the specified metric override must be enabled. For       example, if one or more spark:driver metrics are provided as metric       overrides, the spark metric source must be enabled       (--metric-sources=spark).       --metric-overrides-file=METRIC_OVERRIDES_FILE       Path to a file containing list of Metrics that override the default       metrics enabled for the metric sources. The path can be a Cloud       Storage URL (example: gs://path/to/file) or a local file system path.      At most one of these can be specified:       --no-address       If provided, the instances in the cluster will not be assigned       external IP addresses.         If omitted, then the Dataproc service will apply a default policy to       determine if each instance in the cluster gets an external IP address       or not.         Note: Dataproc VMs need access to the Dataproc API. This can be       achieved without external IP addresses using Private Google Access       (https://cloud.google.com/compute/docs/private-google-access).       --public-ip-address       If provided, cluster instances are assigned external IP addresses.         If omitted, the Dataproc service applies a default policy to       determine whether or not each instance in the cluster gets an       external IP address.         Note: Dataproc VMs need access to the Dataproc API. This can be       achieved without external IP addresses using Private Google Access       (https://cloud.google.com/compute/docs/private-google-access).      At most one of these can be specified:       --single-node       Create a single node cluster.         A single node cluster has all master and worker components. It cannot       have any separate worker nodes. If this flag is not specified, a       cluster with separate workers is created.       Multi-node cluster flags        --min-num-workers=MIN_NUM_WORKERS        Minimum number of primary worker nodes to provision for cluster        creation to succeed.        --num-secondary-workers=NUM_SECONDARY_WORKERS        The number of secondary worker nodes in the cluster.        --num-workers=NUM_WORKERS        The number of worker nodes in the cluster. Defaults to        server-specified.        --secondary-worker-type=TYPE; default="preemptible"        The type of the secondary worker group. TYPE must be one of:        preemptible, non-preemptible, spot.      At most one of these can be specified:       --worker-machine-type=WORKER_MACHINE_TYPE       The type of machine to use for primary workers. Defaults to       server-specified.       --worker-machine-types=type=MACHINE_TYPE[,type=MACHINE_TYPE...][,rank=RANK]       Machine types       (https://cloud.google.com/dataproc/docs/concepts/compute/supported-machine-types)       for primary worker nodes to use with optional rank. A lower rank       number is given higher preference. Based on availablilty, Dataproc       tries to create primary worker VMs using the worker machine type with       the lowest rank, and then tries to use machine types with higher       ranks as necessary. Machine types with the same rank are given the       same preference. Example use:       --worker-machine-types="type=e2-standard-8,type=n2-standard-8,rank=0".       For more information, see Dataproc Flexible VMs       (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/flexible-vms)
    /// </summary>
    [CliOption("--metric-sources", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public GcloudMetricSources? MetricSources { get; set; }

}
