// <auto-generated>
// This file was generated by ModularPipelines.OptionsGenerator on 2025-12-08.
// Source: https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/spark-sql
// Do not edit this file manually.
// </auto-generated>

#nullable enable

using System.Diagnostics.CodeAnalysis;
using ModularPipelines.Attributes;
using ModularPipelines.Google.Options;
using ModularPipelines.Models;

namespace ModularPipelines.Google.Options;

/// <summary>
/// submit a Spark SQL batch job
/// </summary>
[ExcludeFromCodeCoverage]
[CliSubCommand("dataproc", "batches", "submit", "spark-sql")]
public record GcloudDataprocBatchesSubmitSparkSqlOptions(
    [property: CliArgument(0, Name = "SQL_SCRIPT")] string SqlScript
) : GcloudOptions
{
    /// <summary>
    /// Return immediately without waiting for the operation in progress to      complete.
    /// </summary>
    [CliFlag("--async")]
    public bool? Async { get; set; }

    /// <summary>
    /// The ID of the batch job to submit. The ID must contain only lowercase      letters (a-z), numbers (0-9) and hyphens (-). The length of the name      must be between 4 and 63 characters. If this argument is not provided,      a random generated UUID will be used.
    /// </summary>
    [CliOption("--batch", Format = OptionFormat.EqualsSeparated)]
    public string? Batch { get; set; }

    /// <summary>
    /// Optional custom container image to use for the batch/session runtime      environment. If not specified, a default container image will be used.      The value should follow the container image naming format:      {registry}/{repository}/{name}:{tag}, for example,      gcr.io/my-project/my-image:1.2.3
    /// </summary>
    [CliOption("--container-image", Format = OptionFormat.EqualsSeparated)]
    public string? ContainerImage { get; set; }

    /// <summary>
    /// A Cloud Storage bucket to upload workload dependencies.
    /// </summary>
    [CliOption("--deps-bucket", Format = OptionFormat.EqualsSeparated)]
    public string? DepsBucket { get; set; }

    /// <summary>
    /// Spark History Server configuration for the batch/session job. Resource      name of an existing Dataproc cluster to act as a Spark History Server      for the workload in the format:      "projects/{project_id}/regions/{region}/clusters/{cluster_name}".
    /// </summary>
    [CliOption("--history-server-cluster", Format = OptionFormat.EqualsSeparated)]
    public string? HistoryServerCluster { get; set; }

    /// <summary>
    /// Comma-separated list of jar files to be provided to the classpaths.
    /// </summary>
    [CliOption("--jars", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? Jars { get; set; }

    /// <summary>
    /// Cloud KMS key to use for encryption.
    /// </summary>
    [CliOption("--kms-key", Format = OptionFormat.EqualsSeparated)]
    public string? KmsKey { get; set; }

    /// <summary>
    /// List of label KEY=VALUE pairs to add.        Keys must start with a lowercase character and contain only hyphens      (-), underscores (_), lowercase characters, and numbers. Values must      contain only hyphens (-), underscores (_), lowercase characters, and      numbers.
    /// </summary>
    [CliOption("--labels", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Labels { get; set; }

    /// <summary>
    /// Name of a Dataproc Metastore service to be used as an external      metastore in the format:      "projects/{project-id}/locations/{region}/services/{service-name}".
    /// </summary>
    [CliOption("--metastore-service", Format = OptionFormat.EqualsSeparated)]
    public string? MetastoreService { get; set; }

    /// <summary>
    /// Specifies configuration properties for the workload. See Dataproc      Serverless for Spark documentation      (https://cloud.google.com/dataproc-serverless/docs/concepts/properties)      for the list of supported properties.      Region resource - Dataproc region to use. Each Dataproc region constitutes    an independent resource namespace constrained to deploying instances into    Compute Engine zones inside the region. This represents a Cloud resource.    (NOTE) Some attributes are not given arguments in this group but can be    set in other ways.      To set the project attribute:     * provide the argument --region on the command line with a fully      specified name;     * set the property dataproc/region with a fully specified name;     * provide the argument --project on the command line;     * set the property core/project.       --region=REGION       ID of the region or fully qualified identifier for the region.         To set the region attribute:       + provide the argument --region on the command line;       + set the property dataproc/region.
    /// </summary>
    [CliOption("--properties", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Properties { get; set; }

    /// <summary>
    /// A unique ID that identifies the request. If the service receives two      batch create requests with the same request_id, the second request is      ignored and the operation that corresponds to the first batch created      and stored in the backend is returned. Recommendation: Always set this      value to a UUID. The value must contain only letters (a-z, A-Z),      numbers (0-9), underscores (), and hyphens (-). The maximum length is      40 characters.
    /// </summary>
    [CliOption("--request-id", Format = OptionFormat.EqualsSeparated)]
    public string? RequestId { get; set; }

    /// <summary>
    /// The IAM service account to be used for a batch/session job.
    /// </summary>
    [CliOption("--service-account", Format = OptionFormat.EqualsSeparated)]
    public int? ServiceAccount { get; set; }

    /// <summary>
    /// The Cloud Storage bucket to use to store job dependencies, config      files, and job driver console output. If not specified, the default      [staging bucket]      (https://cloud.google.com/dataproc-serverless/docs/concepts/buckets) is      used.
    /// </summary>
    [CliOption("--staging-bucket", Format = OptionFormat.EqualsSeparated)]
    public string? StagingBucket { get; set; }

    /// <summary>
    /// Network tags for traffic control.
    /// </summary>
    [CliOption("--tags", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public IEnumerable<string>? Tags { get; set; }

    /// <summary>
    /// The duration after the workload will be unconditionally terminated, for      example, '20m' or '1h'. Run gcloud topic datetimes      (https://cloud.google.com/sdk/gcloud/reference/topic/datetimes) for      information on duration formats.
    /// </summary>
    [CliOption("--ttl", Format = OptionFormat.EqualsSeparated)]
    public string? Ttl { get; set; }

    /// <summary>
    /// Whether to use END_USER_CREDENTIALS or SERVICE_ACCOUNT to run the      workload.
    /// </summary>
    [CliOption("--user-workload-authentication-type", Format = OptionFormat.EqualsSeparated)]
    public string? UserWorkloadAuthenticationType { get; set; }

    /// <summary>
    /// Mapping of query variable names to values (equivalent to the Spark SQL      command: SET name="value";).
    /// </summary>
    [CliOption("--vars", Format = OptionFormat.EqualsSeparated, AllowMultiple = true)]
    public KeyValue[]? Vars { get; set; }

    /// <summary>
    /// Optional runtime version. If not specified, a default version will be      used.      At most one of these can be specified:       --network=NETWORK       Network URI to connect network to.       --subnet=SUBNET       Subnetwork URI to connect network to. Subnet must have Private Google       Access enabled.
    /// </summary>
    [CliOption("--version", Format = OptionFormat.EqualsSeparated)]
    public string? Version { get; set; }

}
